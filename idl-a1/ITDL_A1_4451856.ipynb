{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dngvpNa45qpZ"
   },
   "source": [
    "# T1.1: MNIST\n",
    "\n",
    "ssh -o ProxyCommand=\"ssh -g -L 8889:localhost:8889 s4451856@sshgw.leidenuniv.nl -q -W U0065090:22\" -g -L 8889:localhost:8889 s4451856@U0065090"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NukGEPhW3rcr"
   },
   "outputs": [],
   "source": [
    "'''Trains a simple convnet on the MNIST dataset.\n",
    "\n",
    "Gets to 99.25% test accuracy after 12 epochs\n",
    "(there is still a lot of margin for parameter tuning).\n",
    "16 seconds per epoch on a GRID K520 GPU.\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Loss\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mnist_cnn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0jXhNZavvqm1",
    "outputId": "957e09f8-dae3-4bbe-b41a-05e6e539dd9f"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 50\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 451
    },
    "id": "TBGrzYyw-ruj",
    "outputId": "8dfa072e-b25e-4b79-f617-674473591371"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xCSx5oxpItwv"
   },
   "source": [
    "## mnist_mlp.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FGLryCAuItX6"
   },
   "outputs": [],
   "source": [
    "'''Trains a simple deep NN on the MNIST dataset.\n",
    "\n",
    "Gets to 98.40% test accuracy after 20 epochs\n",
    "(there is *a lot* of margin for parameter tuning).\n",
    "2 seconds per epoch on a K520 GPU.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "QOopey4iJTAS",
    "outputId": "cac59d72-8f9f-4eab-f56c-b190f5bc86a3"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 20\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_shape=(784,)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 451
    },
    "id": "b7f9ISJCJXqn",
    "outputId": "f71d0dbe-d84c-4557-f5c7-8a9c7fa801d8"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2wUM1haUKc2q"
   },
   "source": [
    "# T1.2: Fashion MNIST\n",
    "\n",
    "## (a) Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nkWfjHSuKvVq"
   },
   "outputs": [],
   "source": [
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(xTrainFull, yTrainFull), (xTest, yTest) = fashion_mnist.load_data()\n",
    "\n",
    "xVal, xTrain = xTrainFull[:5000] / 255.0, xTrainFull[5000:] / 255.0\n",
    "yVal, yTrain = yTrainFull[:5000], yTrainFull[5000:]\n",
    "\n",
    "classNames = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    " \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
    "\n",
    "\n",
    "def MLPModelTest(xTrain, yTrain, xVal, yVal, xTest, yTest,\n",
    "                 learnRate=0.0001,\n",
    "                 hiddenLayers=1,\n",
    "                 outputActivation=\"softmax\",\n",
    "                 hiddenActivation=\"relu\",\n",
    "                 optimiser=\"sgd\",\n",
    "                 epochs=20,\n",
    "                 alpha=5):\n",
    "\n",
    "  MLPModel = Sequential()\n",
    "  MLPModel.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "\n",
    "  for i in range(hiddenLayers):\n",
    "    MLPModel.add(keras.layers.Dense(300, activation=hiddenActivation))\n",
    "    \n",
    "    MLPModel.add(keras.layers.Dense(100, activation=hiddenActivation))\n",
    "\n",
    "\n",
    "  MLPModel.add(keras.layers.Dense(10, activation=outputActivation))\n",
    "\n",
    "  MLPModel.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "  optimizer=optimiser(learning_rate=learnRate),\n",
    "  metrics=[\"accuracy\"])\n",
    "\n",
    "  history = MLPModel.fit(xTrain, yTrain, epochs=epochs,\n",
    "                      validation_data=(xVal, yVal))\n",
    "\n",
    "  pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "  plt.grid(True)\n",
    "  plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.show()\n",
    "\n",
    "  test_loss, test_acc = MLPModel.evaluate(xTest,  yTest, verbose=2)\n",
    "  print(f\"Hidden layers: {hiddenLayers}\")\n",
    "  print(f\"Optimiser: {optimiser}\")\n",
    "  print(f\"Output Activation: {outputActivation}\")\n",
    "  print(f\"Hidden Activation: {hiddenActivation}\")\n",
    "  print(f\"Alpha: {alpha}\")\n",
    "  print(f\"Epochs: {epochs}\")\n",
    "  print(f\"Test loss: {test_loss}\")\n",
    "  print(f\"Test accuracy: {test_acc}\")\n",
    "\n",
    "  return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = MLPModelTest(xTrain, yTrain, xVal, yVal, xTest, yTest,\n",
    "                       learnRate= 0.001,\n",
    "                       hiddenLayers=1,\n",
    "                       optimiser=optimizers.Adam,\n",
    "                       outputActivation=\"softplus\",\n",
    "                       hiddenActivation=\"tanh\",\n",
    "                       epochs=30)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NfhBDw0_OqOJ"
   },
   "source": [
    "## (b) Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LruP8ZWdOpZ7"
   },
   "outputs": [],
   "source": [
    "def CNNModelTest(xTrain, yTrain, xVal, yVal, xTest, yTest,\n",
    "                 learnRate=0.0001,\n",
    "                 hiddenLayers=1,\n",
    "                 layerWidth=64,\n",
    "                 outputActivation=\"softmax\",\n",
    "                 hiddenActivation=\"relu\",\n",
    "                 convActivation=\"relu\",\n",
    "                 optimiser=keras.optimizers.SGD,\n",
    "                 epochs=50,\n",
    "                 alpha=5,\n",
    "                 inputShape=(28, 28, 1),\n",
    "                 outputShape=10,\n",
    "                 lossFunction=keras.losses.SparseCategoricalCrossentropy(from_logits=True)):\n",
    "\n",
    "  CNNModel = Sequential()\n",
    "  CNNModel.add(Conv2D(32, (3, 3), activation=convActivation, input_shape=inputShape))\n",
    "  CNNModel.add(MaxPooling2D((2, 2)))\n",
    "  CNNModel.add(Conv2D(64, (3, 3), activation=convActivation))\n",
    "  CNNModel.add(MaxPooling2D((2, 2)))\n",
    "  CNNModel.add(Conv2D(128, (3, 3), activation=convActivation))\n",
    "  CNNModel.add(MaxPooling2D((2, 2)))\n",
    "  CNNModel.add(Conv2D(256, (3, 3), activation=convActivation))\n",
    "\n",
    "  CNNModel.add(Flatten())\n",
    "\n",
    "  for i in range(hiddenLayers):\n",
    "    CNNModel.add(Dense(layerWidth, activation=hiddenActivation))\n",
    "\n",
    "  CNNModel.add(Dense(640, activation=hiddenActivation))\n",
    "\n",
    "  CNNModel.add(Dense(outputShape, activation=outputActivation))\n",
    "\n",
    "  CNNModel.compile(optimizer=optimiser(learning_rate=learnRate),\n",
    "                loss=lossFunction,\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "  #CNNModel.summary()\n",
    "  history = CNNModel.fit(xTrain, yTrain, epochs=epochs,\n",
    "                      validation_data=(xVal, yVal))\n",
    "  \n",
    "  print(history.history)\n",
    "\n",
    "  pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "  plt.grid(True)\n",
    "  plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.show()\n",
    "\n",
    "  #print(CNNModel.predict(xVal))\n",
    "\n",
    "  val_loss, val_acc = CNNModel.evaluate(xVal,  yVal, verbose=2)\n",
    "  test_loss, test_acc = CNNModel.evaluate(xTest,  yTest, verbose=2)\n",
    "\n",
    "  print(f\"Learning Rate: {learnRate}\")\n",
    "  print(f\"Hidden layers: {hiddenLayers}\")\n",
    "  print(f\"Layer Width: {layerWidth}\")\n",
    "  print(f\"Optimiser: {optimiser}\")\n",
    "  print(f\"Output Activation: {outputActivation}\")\n",
    "  print(f\"Hidden Activation: {hiddenActivation}\")\n",
    "  print(f\"Conv Activation: {convActivation}\")\n",
    "  #print(f\"Alpha: {alpha}\")\n",
    "  print(f\"Epochs: {epochs}\")\n",
    "  print(f\"Validation loss: {val_loss}\")\n",
    "  print(f\"Validation accuracy: {val_acc}\")\n",
    "  print(f\"Test loss: {test_loss}\")\n",
    "  print(f\"Test accuracy: {test_acc}\")\n",
    "\n",
    "  return test_acc, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = CNNModelTest(xTrain, yTrain, xVal, yVal, xTest, yTest,\n",
    "                       learnRate=0.001,\n",
    "                       hiddenLayers=2,\n",
    "                       optimiser=optimizers.Adamax,\n",
    "                       outputActivation=\"elu\",\n",
    "                       hiddenActivation=\"sigmoid\",\n",
    "                       convActivation=\"relu\",\n",
    "                       epochs=30)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class circularLoss(Loss):\n",
    "    def call(self, y_true, y_pred):\n",
    "        return tf.reduce_mean(tf.abs(tf.atan2(tf.sin(y_true - y_pred), tf.cos(y_true - y_pred))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, xTrain, yTrain, xVal, yVal, xTest, yTest, CNN=False,\n",
    "         learnRate=0.01,\n",
    "        hiddenLayers=6,\n",
    "        layerWidth=256,\n",
    "        outputActivation=\"elu\",\n",
    "        hiddenActivation=\"elu\",\n",
    "        convActivation=\"relu\",\n",
    "        epochs=200,\n",
    "        optimiser=keras.optimizers.Adagrad,\n",
    "        inputShape=(75, 75, 1),\n",
    "        outputShape=720,\n",
    "        lossFunction=keras.losses.SparseCategoricalCrossentropy(from_logits=True)):\n",
    "    \n",
    "    # LearnRate\n",
    "    bestLearnRate = (0.01, 0, 0)\n",
    "    #for learnRate in [0.1, 0.01, 0.001, 0.0001, 0.00001, 0.000001]:\n",
    "    #    test_acc, val_acc = model(xTrain, yTrain, xVal, yVal, xTest, yTest,\n",
    "    #                              inputShape=inputShape, outputShape=outputShape,\n",
    "    #                              lossFunction=lossFunction,\n",
    "    #                              learnRate=learnRate)\n",
    "\n",
    "    #    if val_acc > bestLearnRate[2]:\n",
    "    #        bestLearnRate = (learnRate, test_acc, val_acc)\n",
    "\n",
    "    #    elif val_acc < bestLearnRate[2] - 0.1:\n",
    "    #        break\n",
    "\n",
    "\n",
    "    #bestLoss = (circularLoss, 0, 0)\n",
    "    #for loss in [circularLoss,\n",
    "    #                keras.losses.MeanSquaredError,\n",
    "    #                keras.losses.MeanAbsoluteError,\n",
    "    #                keras.losses.MeanSquaredLogarithmicError,\n",
    "    #                keras.losses.MeanAbsolutePercentageError,\n",
    "    #                keras.losses.CosineSimilarity,\n",
    "    #                keras.losses.Huber,\n",
    "    #                keras.losses.LogCosh,\n",
    "    #                keras.losses.Tversky,\n",
    "    #                keras.losses.Dice]:\n",
    "    #    test_acc, val_acc = model(xTrain, yTrain, xVal, yVal, xTest, yTest,\n",
    "    #                              inputShape=inputShape, outputShape=outputShape,\n",
    "    #                              lossFunction=loss,\n",
    "    #                              learnRate=bestLearnRate[0],\n",
    "    #                            epochs=100)\n",
    "\n",
    "    #    if val_acc > bestLoss[2]:\n",
    "    #        bestLoss = (loss, test_acc, val_acc)\n",
    "\n",
    "    \n",
    "    # Epochs\n",
    "    bestEpochs = (125, 0, 0)\n",
    "    #for epochs in range(25, 250, 25):\n",
    "    #    test_acc, val_acc = model(xTrain, yTrain, xVal, yVal, xTest, yTest,\n",
    "    #                              inputShape=inputShape, outputShape=outputShape,\n",
    "    #                              lossFunction=lossFunction,\n",
    "    #                              learnRate=learnRate,\n",
    "    #                              hiddenLayers=hiddenLayers,\n",
    "    #                              layerWidth=layerWidth,\n",
    "    #                              outputActivation=outputActivation,\n",
    "    #                              hiddenActivation=hiddenActivation,\n",
    "    #                              convActivation=convActivation,\n",
    "    #                              epochs=epochs,\n",
    "    #                              optimiser=optimiser)\n",
    "\n",
    "    #    if val_acc > bestEpochs[2]:\n",
    "    #       bestEpochs = (epochs, test_acc, val_acc)\n",
    "\n",
    "    #    else:\n",
    "    #        break\n",
    "\n",
    "        #elif val_acc < bestEpochs[2] - 0.1:\n",
    "        #    break\n",
    "\n",
    "\n",
    "    # Optimiser\n",
    "    bestOptimiser = (keras.optimizers.Adagrad, 0, 0)\n",
    "    #for optimiser in [keras.optimizers.SGD,\n",
    "    #                keras.optimizers.RMSprop,\n",
    "    #                keras.optimizers.Adam,\n",
    "    #                keras.optimizers.AdamW,\n",
    "    #                keras.optimizers.Adadelta,\n",
    "    #                keras.optimizers.Adagrad,\n",
    "    #                keras.optimizers.Adamax,\n",
    "    #                keras.optimizers.Adafactor,\n",
    "    #                keras.optimizers.Nadam,\n",
    "    #                keras.optimizers.Ftrl,\n",
    "    #                keras.optimizers.Lion]:\n",
    "    #    test_acc, val_acc = model(xTrain, yTrain, xVal, yVal, xTest, yTest,\n",
    "    #                              inputShape=inputShape, outputShape=outputShape,\n",
    "    #                              lossFunction=lossFunction,\n",
    "    #                              learnRate=learnRate,\n",
    "    #                              hiddenLayers=hiddenLayers,\n",
    "    #                              layerWidth=layerWidth,\n",
    "    #                              outputActivation=outputActivation,\n",
    "    #                              hiddenActivation=hiddenActivation,\n",
    "    #                              convActivation=convActivation,\n",
    "    #                              epochs=bestEpochs[0],\n",
    "    #                              optimiser=optimiser)\n",
    "\n",
    "    #    if val_acc > bestOptimiser[2]:\n",
    "    #        bestOptimiser = (optimiser, test_acc, val_acc)\n",
    "\n",
    "\n",
    "    # Output activation function\n",
    "    bestOutActivation = (\"selu\", 0, 0)\n",
    "    #for outputActivation in [\"relu\", \"sigmoid\", \"softmax\", \"softplus\", \"softsign\",\n",
    "    #                \"tanh\", \"selu\", \"elu\", \"exponential\"]:\n",
    "    #    test_acc, val_acc = model(xTrain, yTrain, xVal, yVal, xTest, yTest,\n",
    "    #                              inputShape=inputShape, outputShape=outputShape,\n",
    "    #                              lossFunction=lossFunction,\n",
    "    #                              learnRate=learnRate,\n",
    "    #                              hiddenLayers=hiddenLayers,\n",
    "    #                              layerWidth=layerWidth,\n",
    "    #                              outputActivation=outputActivation,\n",
    "    #                              hiddenActivation=hiddenActivation,\n",
    "    #                              convActivation=convActivation,\n",
    "    #                              epochs=bestEpochs[0],\n",
    "    #                              optimiser=bestOptimiser[0])\n",
    "\n",
    "    #    if val_acc > bestOutActivation[2]:\n",
    "    #        bestOutActivation = (outputActivation, test_acc, val_acc)\n",
    "\n",
    "\n",
    "    # Hidden Layer activation function\n",
    "    bestHiddenActivation = (\"softsign\", 0, 0)\n",
    "    #for hiddenActivation in [\"relu\", \"sigmoid\", \"softmax\", \"softplus\", \"softsign\",\n",
    "    #                \"tanh\", \"selu\", \"elu\", \"exponential\"]:\n",
    "    #    test_acc, val_acc = model(xTrain, yTrain, xVal, yVal, xTest, yTest,\n",
    "    #                              inputShape=inputShape, outputShape=outputShape,\n",
    "    #                              lossFunction=lossFunction,\n",
    "    #                              learnRate=learnRate,\n",
    "    #                              hiddenLayers=hiddenLayers,\n",
    "    #                              layerWidth=layerWidth,\n",
    "    #                              outputActivation=bestOutActivation[0],\n",
    "    #                              hiddenActivation=hiddenActivation,\n",
    "    #                              convActivation=convActivation,\n",
    "    #                              epochs=bestEpochs[0],\n",
    "    #                              optimiser=bestOptimiser[0])\n",
    "\n",
    "    #    if val_acc > bestHiddenActivation[2]:\n",
    "    #        bestHiddenActivation = (hiddenActivation, test_acc, val_acc)\n",
    "\n",
    "    \n",
    "    # Convolutional layer Activation Function\n",
    "    bestConvActivation = (\"relu\", 0, 0)\n",
    "    #if CNN:\n",
    "    #    for convActivation in [\"relu\", \"sigmoid\", \"softmax\", \"softplus\", \"softsign\",\n",
    "    #                    \"tanh\", \"selu\", \"elu\", \"exponential\"]:\n",
    "    #        test_acc, val_acc = CNNModelTest(xTrain, yTrain, xVal, yVal, xTest, yTest,\n",
    "    #                                         inputShape=inputShape, outputShape=outputShape,\n",
    "    #                                         lossFunction=lossFunction,\n",
    "    #                                        learnRate=learnRate,\n",
    "    #                                        hiddenLayers=hiddenLayers,\n",
    "    #                                        layerWidth=layerWidth,\n",
    "    #                                        outputActivation=bestOutActivation[0],\n",
    "    #                                        hiddenActivation=bestHiddenActivation[0],\n",
    "    #                                        convActivation=convActivation,\n",
    "    #                                        epochs=bestEpochs[0],\n",
    "    #                                        optimiser=bestOptimiser[0])\n",
    "\n",
    "    #        if val_acc > bestConvActivation[2]:\n",
    "    #            bestConvActivation = (convActivation, test_acc, val_acc)\n",
    "\n",
    "    \n",
    "    # Hidden Layers\n",
    "    bestHidden = (6, 0, 0)\n",
    "    #for hiddenLayers in range(1, 50):\n",
    "    #    test_acc, val_acc = model(xTrain, yTrain, xVal, yVal, xTest, yTest,\n",
    "    #                              inputShape=inputShape, outputShape=outputShape,\n",
    "    #                            lossFunction=lossFunction,\n",
    "    #                            learnRate=learnRate,\n",
    "    #                            hiddenLayers=hiddenLayers,\n",
    "    #                            layerWidth=layerWidth,\n",
    "    #                            outputActivation=bestOutActivation[0],\n",
    "    #                            hiddenActivation=bestHiddenActivation[0],\n",
    "    #                            convActivation=bestConvActivation[0],\n",
    "    #                            epochs=bestEpochs[0],\n",
    "    #                            optimiser=bestOptimiser[0])\n",
    "\n",
    "    #    if val_acc > bestHidden[2]:\n",
    "    #        bestHidden = (hiddenLayers, test_acc, val_acc)\n",
    "\n",
    "    #    elif val_acc < bestHidden[2] - 0.1:\n",
    "    #        break\n",
    "\n",
    "    \n",
    "    # Hidden Layers\n",
    "    bestWidth = (1, 0, 0)\n",
    "    accuracyList = []\n",
    "    for layerWidth in range(256, 640, 8):\n",
    "        test_acc, val_acc = model(xTrain, yTrain, xVal, yVal, xTest, yTest,\n",
    "                                  inputShape=inputShape, outputShape=outputShape,\n",
    "                                lossFunction=lossFunction,\n",
    "                                learnRate=learnRate,\n",
    "                                hiddenLayers=bestHidden[0],\n",
    "                                layerWidth=layerWidth,\n",
    "                                outputActivation=bestOutActivation[0],\n",
    "                                hiddenActivation=bestHiddenActivation[0],\n",
    "                                convActivation=bestConvActivation[0],\n",
    "                                epochs=bestEpochs[0],\n",
    "                                optimiser=bestOptimiser[0])\n",
    "    \n",
    "        accuracyList.append(val_acc)\n",
    "\n",
    "        if val_acc > bestWidth[2]:\n",
    "            bestWidth = (layerWidth, test_acc, val_acc)\n",
    "\n",
    "        elif val_acc < bestWidth[2] - 0.1:\n",
    "            break\n",
    "\n",
    "\n",
    "    # Regularisations\n",
    "    bestAlpha = (0, 0, 0)\n",
    "    #alphas = np.logspace(-10, -2, 200)\n",
    "    #for alpha in alphas:\n",
    "    #  test_acc, val_acc = model(xTrain, yTrain, xVal, yVal, xTest, yTest,\n",
    "    #                          hiddenLayers=bestHidden[0],\n",
    "    #                          outputActivation=bestOutActivation[0],\n",
    "    #                          hiddenActivation=bestHiddenActivation[0],\n",
    "    #                          epochs=bestEpochs[0],\n",
    "    #                          optimiser=bestOptimiser[0],\n",
    "    #                          alpha=alpha)\n",
    "\n",
    "    #  if accuracy > bestAlpha[2]:\n",
    "    #   bestAlpha = (alpha, accuracy)\n",
    "\n",
    "    print(f\"Best Learning Rate: {bestLearnRate[0]}\")\n",
    "    print(f\"Best Hidden layers: {bestHidden[0]}\")\n",
    "    print(f\"Best Width: {bestWidth[0]}\")\n",
    "    print(f\"Best Optimiser: {bestOptimiser[0]}\")\n",
    "    print(f\"Best Output Activation: {bestOutActivation[0]}\")\n",
    "    print(f\"Best Hidden Activation: {bestHiddenActivation[0]}\")\n",
    "    print(f\"Best Conv Activation: {bestConvActivation[0]}\")\n",
    "    print(f\"Best Epochs: {bestEpochs[0]}\")\n",
    "    print(f\"Final Val accuracy: {bestHidden[2]}\")\n",
    "    print(f\"Final Test accuracy: {bestHidden[1]}\")\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(accuracyList, range(len(accuracyList)))\n",
    "    plt.show()\n",
    "\n",
    "    return {\"bestHidden\": bestHidden[0], \"bestOptimiser\": bestOptimiser[0],\n",
    "            \"bestOutActivation\": bestOutActivation[0],\n",
    "            \"bestHiddenActivation\": bestOutActivation[0],\n",
    "            \"bestEpochs\": bestEpochs[0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentation and Evaluation\n",
    "\n",
    "### MLP Model Evaluation\n",
    "\n",
    "Naive approach, best hyperparameters:\n",
    "\n",
    "Hidden layers: 1\n",
    "\n",
    "Optimiser: Adam\n",
    "\n",
    "Output Activation: SoftPlus\n",
    "\n",
    "Hidden Activation: Tanh\n",
    "\n",
    "Alpha:  5\n",
    "\n",
    "Epochs: 30\n",
    "\n",
    "Test accuracy: 0.8705999851226807"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "qOSRqMNMGoH3",
    "outputId": "31ded925-fc5d-4a5a-9692-8eb146673a6a"
   },
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(xTrainFull, yTrainFull), (xTest, yTest) = fashion_mnist.load_data()\n",
    "\n",
    "xVal, xTrain = xTrainFull[:5000] / 255.0, xTrainFull[5000:] / 255.0\n",
    "yVal, yTrain = yTrainFull[:5000], yTrainFull[5000:]\n",
    "\n",
    "# MLP model eval\n",
    "bestSettings = eval(MLPModelTest, xTrain, yTrain, xVal, yVal, xTest, yTest)\n",
    "print(bestSettings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Model Evaluation\n",
    "\n",
    "Naive approach, best hyperparameters:\n",
    "\n",
    "Hidden layers: 2\n",
    "\n",
    "Optimiser: Adamax\n",
    "\n",
    "Output Activation: elu\n",
    "\n",
    "Hidden Activation: sigmoid\n",
    "\n",
    "Conv Activation: relu\n",
    "\n",
    "Epochs: 30\n",
    "\n",
    "Test loss: 0.3733844459056854\n",
    "\n",
    "Test accuracy: 0.8733000159263611"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN model eval\n",
    "bestSettings = eval(CNNModelTest, xTrain, yTrain, xVal, yVal, xTest, yTest, CNN=True)\n",
    "print(bestSettings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar = keras.datasets.cifar10\n",
    "(xTrainFull, yTrainFull), (xTest, yTest) = cifar.load_data()\n",
    "\n",
    "xVal, xTrain = xTrainFull[:5000] / 255.0, xTrainFull[5000:] / 255.0\n",
    "yVal, yTrain = yTrainFull[:5000], yTrainFull[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hidden layers: 2\n",
    "#Optimiser: Adamax\n",
    "#Output Activation: elu\n",
    "#Hidden Activation: sigmoid\n",
    "#Conv Activation: relu\n",
    "#Epochs: 30\n",
    "#Test loss: 0.3733844459056854\n",
    "#Test accuracy: 0.8733000159263611\n",
    "\n",
    "test_acc, val_acc = CNNModelTest(xTrain, yTrain, xVal, yVal, xTest, yTest,\n",
    "                            learnRate=0.001,\n",
    "                          hiddenLayers=2,\n",
    "                          outputActivation=\"elu\",\n",
    "                          hiddenActivation=\"sigmoid\",\n",
    "                          convActivation=\"relu\",\n",
    "                          epochs=30,\n",
    "                          optimiser=keras.optimizers.Adamax,\n",
    "                          inputShape=(32, 32, 3))\n",
    "\n",
    "print(test_acc, val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T2.1 Tell-the-time Network\n",
    "\n",
    "## (a) Classification\n",
    "\n",
    "24 Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xSmallClock = np.load(\"data/75/images.npy\")\n",
    "ySmallClock = np.load(\"data/75/labels.npy\")\n",
    "\n",
    "ySmallClock = np.array([int(time[0] + (time[1] // 30)) for time in ySmallClock])  \n",
    "\n",
    "xSmallClock = np.reshape(xSmallClock, (-1, 75, 75, 1))\n",
    "xSmallClock = xSmallClock / 255.0\n",
    "\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(xSmallClock,\n",
    "                                                ySmallClock,\n",
    "                                                test_size=0.2)\n",
    "xVal, xTest, yVal, yTest = train_test_split(xTest, yTest, test_size=0.5)\n",
    "\n",
    "test_acc, val_acc = CNNModelTest(xTrain, yTrain, xVal, yVal, xTest, yTest,\n",
    "                                 learnRate=0.001,\n",
    "                                hiddenLayers=2,\n",
    "                                outputActivation=\"softmax\",\n",
    "                                hiddenActivation=\"relu\",\n",
    "                                convActivation=\"relu\",\n",
    "                                epochs=30,\n",
    "                                optimiser=keras.optimizers.Adam,\n",
    "                                inputShape=(75, 75, 1),\n",
    "                                outputShape=24)\n",
    "\n",
    "print(test_acc, val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "720 Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xSmallClock = np.load(\"data/75/images.npy\")\n",
    "ySmallClock = np.load(\"data/75/labels.npy\")\n",
    "\n",
    "ySmallClock = np.array([int((time[0] * 60) + time[1]) for time in ySmallClock])  \n",
    "\n",
    "xSmallClock = np.reshape(xSmallClock, (-1, 75, 75, 1))\n",
    "xSmallClock = xSmallClock / 255.0\n",
    "\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(xSmallClock,\n",
    "                                                ySmallClock,\n",
    "                                                test_size=0.2)\n",
    "xVal, xTest, yVal, yTest = train_test_split(xTest, yTest, test_size=0.5)\n",
    "\n",
    "#test_acc, val_acc = CNNModelTest(xTrain, yTrain, xVal, yVal, xTest, yTest,\n",
    "#                          learnRate=0.01,\n",
    "#                          hiddenLayers=6,\n",
    "#                          layerWidth=256,\n",
    "#                          outputActivation=\"elu\",\n",
    "#                          hiddenActivation=\"elu\",\n",
    "#                          convActivation=\"relu\",\n",
    "#                          epochs=200,\n",
    "#                          optimiser=keras.optimizers.Adagrad,\n",
    "#                          inputShape=(75, 75, 1),\n",
    "#                          outputShape=720)\n",
    "\n",
    "#print(test_acc, val_acc)\n",
    "\n",
    "bestSettings = eval(CNNModelTest, xTrain, yTrain, xVal, yVal, xTest, yTest,\n",
    "                          learnRate=0.01,\n",
    "                          hiddenLayers=6,\n",
    "                          layerWidth=512,\n",
    "                          outputActivation=\"elu\",\n",
    "                          hiddenActivation=\"elu\",\n",
    "                          convActivation=\"relu\",\n",
    "                          epochs=200,\n",
    "                          optimiser=keras.optimizers.Adagrad,\n",
    "                          inputShape=(75, 75, 1),\n",
    "                          outputShape=720,\n",
    "                          lossFunction=keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
    "\n",
    "print(bestSettings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xSmallClock = np.load(\"data/75/images.npy\")\n",
    "ySmallClock = np.load(\"data/75/labels.npy\")\n",
    "\n",
    "ySmallClock = np.array([float(time[0] + (time[1] / 60)) for time in ySmallClock])\n",
    "\n",
    "xSmallClock = np.reshape(xSmallClock, (-1, 75, 75, 1))\n",
    "xSmallClock = xSmallClock / 255.0\n",
    "    \n",
    "xTrain, xTest, yTrain, yTest = train_test_split(xSmallClock,\n",
    "                                                ySmallClock,\n",
    "                                                test_size=0.2)\n",
    "xVal, xTest, yVal, yTest = train_test_split(xTest, yTest, test_size=0.5)\n",
    "\n",
    "test_acc, val_acc = CNNModelTest(xTrain, yTrain, xVal, yVal, xTest, yTest,\n",
    "                                 learnRate=0.01,\n",
    "                                hiddenLayers=50,\n",
    "                                layerWidth=8,\n",
    "                                outputActivation=\"softmax\",\n",
    "                                hiddenActivation=\"relu\",\n",
    "                                convActivation=\"relu\",\n",
    "                                epochs=200,\n",
    "                                optimiser=keras.optimizers.Adam,\n",
    "                                inputShape=(75, 75, 1),\n",
    "                                outputShape=1,\n",
    "                                lossFunction=keras.losses.mean_absolute_error)\n",
    "\n",
    "print(test_acc, val_acc)\n",
    "\n",
    "#bestSettings = eval(CNNModelTest, xTrain, yTrain, xVal, yVal, xTest, yTest, CNN=True,\n",
    "#                    inputShape=(75, 75, 1), outputShape=1, lossFunction=circularLoss)\n",
    "#print(bestSettings)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
