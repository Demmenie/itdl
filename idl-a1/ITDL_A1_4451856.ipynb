{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dngvpNa45qpZ"
   },
   "source": [
    "# T1.1: MNIST\n",
    "\n",
    "ssh -o ProxyCommand=\"ssh -g -L 8889:localhost:8889 s4451856@sshgw.leidenuniv.nl -q -W U0065090:22\" -g -L 8889:localhost:8889 s4451856@U0065090"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mnist_cnn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0jXhNZavvqm1",
    "outputId": "957e09f8-dae3-4bbe-b41a-05e6e539dd9f"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 50\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 451
    },
    "id": "TBGrzYyw-ruj",
    "outputId": "8dfa072e-b25e-4b79-f617-674473591371"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xCSx5oxpItwv"
   },
   "source": [
    "## mnist_mlp.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FGLryCAuItX6"
   },
   "outputs": [],
   "source": [
    "'''Trains a simple deep NN on the MNIST dataset.\n",
    "\n",
    "Gets to 98.40% test accuracy after 20 epochs\n",
    "(there is *a lot* of margin for parameter tuning).\n",
    "2 seconds per epoch on a K520 GPU.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "QOopey4iJTAS",
    "outputId": "cac59d72-8f9f-4eab-f56c-b190f5bc86a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/home/s4451856/miniconda3/envs/IDL/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1731169074.124477 2985350 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1731169074.163651 2985350 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1731169074.163743 2985350 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1731169074.165527 2985350 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1731169074.165597 2985350 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1731169074.165642 2985350 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1731169074.211810 2985350 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1731169074.211922 2985350 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1731169074.211980 2985350 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-09 17:17:54.212023: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6230 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">401,920</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">262,656</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,130</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m401,920\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m262,656\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m5,130\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">669,706</span> (2.55 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m669,706\u001b[0m (2.55 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">669,706</span> (2.55 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m669,706\u001b[0m (2.55 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1731169074.995159 2985534 service.cc:146] XLA service 0x7f2594008590 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1731169074.995178 2985534 service.cc:154]   StreamExecutor device (0): NVIDIA GeForce RTX 4060, Compute Capability 8.9\n",
      "2024-11-09 17:17:55.008761: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-11-09 17:17:55.051495: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 8907\n",
      "2024-11-09 17:17:55.121858: W external/local_xla/xla/service/gpu/nvptx_compiler.cc:762] The NVIDIA driver's CUDA version is 12.2 which is older than the ptxas CUDA version (12.3.107). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n",
      "I0000 00:00:1731169075.920707 2985534 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 34\u001b[0m\n\u001b[1;32m     28\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n\u001b[1;32m     30\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     31\u001b[0m               optimizer\u001b[38;5;241m=\u001b[39mRMSprop(),\n\u001b[1;32m     32\u001b[0m               metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 34\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(x_train, y_train,\n\u001b[1;32m     35\u001b[0m                     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m     36\u001b[0m                     epochs\u001b[38;5;241m=\u001b[39mepochs,\n\u001b[1;32m     37\u001b[0m                     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     38\u001b[0m                     validation_data\u001b[38;5;241m=\u001b[39m(x_test, y_test))\n\u001b[1;32m     39\u001b[0m score \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(x_test, y_test, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest loss:\u001b[39m\u001b[38;5;124m'\u001b[39m, score[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/IDL/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/IDL/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:320\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[1;32m    319\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 320\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[1;32m    321\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m~/miniconda3/envs/IDL/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/IDL/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/miniconda3/envs/IDL/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:919\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m   \u001b[38;5;66;03m# If we did not create any variables the trace we have is good enough.\u001b[39;00m\n\u001b[1;32m    914\u001b[0m   filtered_flat_args \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    915\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concrete_variable_creation_fn\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(\n\u001b[1;32m    916\u001b[0m           bound_args\n\u001b[1;32m    917\u001b[0m       )\n\u001b[1;32m    918\u001b[0m   )\n\u001b[0;32m--> 919\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concrete_variable_creation_fn\u001b[38;5;241m.\u001b[39m_call_flat(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    920\u001b[0m       filtered_flat_args,\n\u001b[1;32m    921\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concrete_variable_creation_fn\u001b[38;5;241m.\u001b[39mcaptured_inputs,\n\u001b[1;32m    922\u001b[0m   )\n\u001b[1;32m    924\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn_with_cond\u001b[39m(inner_args, inner_kwds):\n\u001b[1;32m    925\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Conditionally runs initialization if it's needed.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/IDL/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function\u001b[38;5;241m.\u001b[39mcall_preflattened(args)\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/miniconda3/envs/IDL/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_flat(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/IDL/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    253\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    254\u001b[0m         \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mflat_outputs),\n\u001b[1;32m    255\u001b[0m     )\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/IDL/lib/python3.12/site-packages/tensorflow/python/eager/context.py:1552\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1550\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1552\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[1;32m   1553\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1554\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   1555\u001b[0m       inputs\u001b[38;5;241m=\u001b[39mtensor_inputs,\n\u001b[1;32m   1556\u001b[0m       attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[1;32m   1557\u001b[0m       ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1558\u001b[0m   )\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1560\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1561\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1562\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1566\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1567\u001b[0m   )\n",
      "File \u001b[0;32m~/miniconda3/envs/IDL/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 20\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_shape=(784,)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 451
    },
    "id": "b7f9ISJCJXqn",
    "outputId": "f71d0dbe-d84c-4557-f5c7-8a9c7fa801d8"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2wUM1haUKc2q"
   },
   "source": [
    "# T1.2: Fashion MNIST\n",
    "\n",
    "## (a) Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "nkWfjHSuKvVq"
   },
   "outputs": [],
   "source": [
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(xTrainFull, yTrainFull), (xTest, yTest) = fashion_mnist.load_data()\n",
    "\n",
    "xVal, xTrain = xTrainFull[:5000] / 255.0, xTrainFull[5000:] / 255.0\n",
    "yVal, yTrain = yTrainFull[:5000], yTrainFull[5000:]\n",
    "\n",
    "classNames = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    " \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
    "\n",
    "\n",
    "def MLPModelTest(xTrain, yTrain, xVal, yVal, xTest, yTest,\n",
    "                 learnRate=0.0001,\n",
    "                 hiddenLayers=1,\n",
    "                 outputActivation=\"softmax\",\n",
    "                 hiddenActivation=\"relu\",\n",
    "                 optimiser=\"sgd\",\n",
    "                 epochs=20,\n",
    "                 alpha=5):\n",
    "\n",
    "  MLPModel = Sequential()\n",
    "  MLPModel.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "\n",
    "  for i in range(hiddenLayers):\n",
    "    MLPModel.add(keras.layers.Dense(300, activation=hiddenActivation))\n",
    "    \n",
    "    MLPModel.add(keras.layers.Dense(100, activation=hiddenActivation))\n",
    "\n",
    "\n",
    "  MLPModel.add(keras.layers.Dense(10, activation=outputActivation))\n",
    "\n",
    "  MLPModel.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "  optimizer=optimiser(learning_rate=learnRate),\n",
    "  metrics=[\"accuracy\"])\n",
    "\n",
    "  history = MLPModel.fit(xTrain, yTrain, epochs=epochs,\n",
    "                      validation_data=(xVal, yVal))\n",
    "\n",
    "  pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "  plt.grid(True)\n",
    "  plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.show()\n",
    "\n",
    "  test_loss, test_acc = MLPModel.evaluate(xTest,  yTest, verbose=2)\n",
    "  print(f\"Hidden layers: {hiddenLayers}\")\n",
    "  print(f\"Optimiser: {optimiser}\")\n",
    "  print(f\"Output Activation: {outputActivation}\")\n",
    "  print(f\"Hidden Activation: {hiddenActivation}\")\n",
    "  print(f\"Alpha: {alpha}\")\n",
    "  print(f\"Epochs: {epochs}\")\n",
    "  print(f\"Test loss: {test_loss}\")\n",
    "  print(f\"Test accuracy: {test_acc}\")\n",
    "\n",
    "  return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = MLPModelTest(xTrain, yTrain, xVal, yVal, xTest, yTest,\n",
    "                       learnRate= 0.001,\n",
    "                       hiddenLayers=1,\n",
    "                       optimiser=optimizers.Adam,\n",
    "                       outputActivation=\"softplus\",\n",
    "                       hiddenActivation=\"tanh\",\n",
    "                       epochs=30)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NfhBDw0_OqOJ"
   },
   "source": [
    "## (b) Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LruP8ZWdOpZ7"
   },
   "outputs": [],
   "source": [
    "def CNNModelTest(xTrain, yTrain, xVal, yVal, xTest, yTest,\n",
    "                 learnRate=0.0001,\n",
    "                 hiddenLayers=1,\n",
    "                 layerWidth=64,\n",
    "                 outputActivation=\"softmax\",\n",
    "                 hiddenActivation=\"relu\",\n",
    "                 convActivation=\"relu\",\n",
    "                 optimiser=keras.optimizers.SGD,\n",
    "                 epochs=50,\n",
    "                 alpha=5,\n",
    "                 inputShape=(28, 28, 1),\n",
    "                 outputShape=10,\n",
    "                 lossFunction=keras.losses.SparseCategoricalCrossentropy(from_logits=True)):\n",
    "\n",
    "  CNNModel = Sequential()\n",
    "  CNNModel.add(Conv2D(32, (3, 3), activation=convActivation, input_shape=inputShape))\n",
    "  CNNModel.add(MaxPooling2D((2, 2)))\n",
    "  CNNModel.add(Conv2D(64, (3, 3), activation=convActivation))\n",
    "  CNNModel.add(MaxPooling2D((2, 2)))\n",
    "  CNNModel.add(Conv2D(128, (3, 3), activation=convActivation))\n",
    "  CNNModel.add(MaxPooling2D((2, 2)))\n",
    "  CNNModel.add(Conv2D(256, (3, 3), activation=convActivation))\n",
    "\n",
    "  CNNModel.add(Flatten())\n",
    "\n",
    "  for i in range(hiddenLayers):\n",
    "    CNNModel.add(Dense(layerWidth, activation=hiddenActivation))\n",
    "\n",
    "  # CNNModel.add(Dense(640, activation=hiddenActivation))\n",
    "\n",
    "  CNNModel.add(Dense(outputShape, activation=outputActivation))\n",
    "\n",
    "  CNNModel.compile(optimizer=optimiser(learning_rate=learnRate),\n",
    "                loss=lossFunction,\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "  #CNNModel.summary()\n",
    "  history = CNNModel.fit(xTrain, yTrain, epochs=epochs,\n",
    "                      validation_data=(xVal, yVal))\n",
    "  \n",
    "  print(history.history)\n",
    "\n",
    "  pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "  plt.grid(True)\n",
    "  plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.show()\n",
    "\n",
    "  #print(CNNModel.predict(xVal))\n",
    "\n",
    "  val_loss, val_acc = CNNModel.evaluate(xVal,  yVal, verbose=2)\n",
    "  test_loss, test_acc = CNNModel.evaluate(xTest,  yTest, verbose=2)\n",
    "\n",
    "  print(f\"Learning Rate: {learnRate}\")\n",
    "  print(f\"Hidden layers: {hiddenLayers}\")\n",
    "  print(f\"Layer Width: {layerWidth}\")\n",
    "  print(f\"Optimiser: {optimiser}\")\n",
    "  print(f\"Output Activation: {outputActivation}\")\n",
    "  print(f\"Hidden Activation: {hiddenActivation}\")\n",
    "  print(f\"Conv Activation: {convActivation}\")\n",
    "  #print(f\"Alpha: {alpha}\")\n",
    "  print(f\"Epochs: {epochs}\")\n",
    "  print(f\"Validation loss: {val_loss}\")\n",
    "  print(f\"Validation accuracy: {val_acc}\")\n",
    "  print(f\"Test loss: {test_loss}\")\n",
    "  print(f\"Test accuracy: {test_acc}\")\n",
    "\n",
    "  return test_acc, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = CNNModelTest(xTrain, yTrain, xVal, yVal, xTest, yTest,\n",
    "                       learnRate=0.001,\n",
    "                       hiddenLayers=2,\n",
    "                       optimiser=optimizers.Adamax,\n",
    "                       outputActivation=\"elu\",\n",
    "                       hiddenActivation=\"sigmoid\",\n",
    "                       convActivation=\"relu\",\n",
    "                       epochs=30)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, xTrain, yTrain, xVal, yVal, xTest, yTest, CNN=False,\n",
    "         learnRate=0.01,\n",
    "        hiddenLayers=6,\n",
    "        layerWidth=256,\n",
    "        outputActivation=\"elu\",\n",
    "        hiddenActivation=\"elu\",\n",
    "        convActivation=\"relu\",\n",
    "        epochs=200,\n",
    "        optimiser=keras.optimizers.Adagrad,\n",
    "        inputShape=(75, 75, 1),\n",
    "        outputShape=720,\n",
    "        lossFunction=keras.losses.SparseCategoricalCrossentropy(from_logits=True)):\n",
    "    \n",
    "    # LearnRate\n",
    "    bestLearnRate = (0.01, 0, 0)\n",
    "    #for learnRate in [0.1, 0.01, 0.001, 0.0001, 0.00001, 0.000001]:\n",
    "    #    test_acc, val_acc = model(xTrain, yTrain, xVal, yVal, xTest, yTest,\n",
    "    #                              inputShape=inputShape, outputShape=outputShape,\n",
    "    #                              lossFunction=lossFunction,\n",
    "    #                              learnRate=learnRate)\n",
    "\n",
    "    #    if val_acc > bestLearnRate[2]:\n",
    "    #        bestLearnRate = (learnRate, test_acc, val_acc)\n",
    "\n",
    "    #    elif val_acc < bestLearnRate[2] - 0.1:\n",
    "    #        break\n",
    "\n",
    "\n",
    "    #bestLoss = (circularLoss, 0, 0)\n",
    "    #for loss in [circularLoss,\n",
    "    #                keras.losses.MeanSquaredError,\n",
    "    #                keras.losses.MeanAbsoluteError,\n",
    "    #                keras.losses.MeanSquaredLogarithmicError,\n",
    "    #                keras.losses.MeanAbsolutePercentageError,\n",
    "    #                keras.losses.CosineSimilarity,\n",
    "    #                keras.losses.Huber,\n",
    "    #                keras.losses.LogCosh,\n",
    "    #                keras.losses.Tversky,\n",
    "    #                keras.losses.Dice]:\n",
    "    #    test_acc, val_acc = model(xTrain, yTrain, xVal, yVal, xTest, yTest,\n",
    "    #                              inputShape=inputShape, outputShape=outputShape,\n",
    "    #                              lossFunction=loss,\n",
    "    #                              learnRate=bestLearnRate[0],\n",
    "    #                            epochs=100)\n",
    "\n",
    "    #    if val_acc > bestLoss[2]:\n",
    "    #        bestLoss = (loss, test_acc, val_acc)\n",
    "\n",
    "    \n",
    "    # Epochs\n",
    "    bestEpochs = (150, 0, 0)\n",
    "    #for epochs in range(25, 250, 25):\n",
    "    #    test_acc, val_acc = model(xTrain, yTrain, xVal, yVal, xTest, yTest,\n",
    "    #                              inputShape=inputShape, outputShape=outputShape,\n",
    "    #                              lossFunction=lossFunction,\n",
    "    #                              learnRate=learnRate,\n",
    "    #                              hiddenLayers=hiddenLayers,\n",
    "    #                              layerWidth=layerWidth,\n",
    "    #                              outputActivation=outputActivation,\n",
    "    #                              hiddenActivation=hiddenActivation,\n",
    "    #                              convActivation=convActivation,\n",
    "    #                              epochs=epochs,\n",
    "    #                              optimiser=optimiser)\n",
    "\n",
    "    #    if val_acc > bestEpochs[2]:\n",
    "    #       bestEpochs = (epochs, test_acc, val_acc)\n",
    "\n",
    "    #    else:\n",
    "    #        break\n",
    "\n",
    "        #elif val_acc < bestEpochs[2] - 0.1:\n",
    "        #    break\n",
    "\n",
    "\n",
    "    # Optimiser\n",
    "    bestOptimiser = (keras.optimizers.Adagrad, 0, 0)\n",
    "    #for optimiser in [keras.optimizers.SGD,\n",
    "    #                keras.optimizers.RMSprop,\n",
    "    #                keras.optimizers.Adam,\n",
    "    #                keras.optimizers.AdamW,\n",
    "    #                keras.optimizers.Adadelta,\n",
    "    #                keras.optimizers.Adagrad,\n",
    "    #                keras.optimizers.Adamax,\n",
    "    #                keras.optimizers.Adafactor,\n",
    "    #                keras.optimizers.Nadam,\n",
    "    #                keras.optimizers.Ftrl,\n",
    "    #                keras.optimizers.Lion]:\n",
    "    #    test_acc, val_acc = model(xTrain, yTrain, xVal, yVal, xTest, yTest,\n",
    "    #                              inputShape=inputShape, outputShape=outputShape,\n",
    "    #                              lossFunction=lossFunction,\n",
    "    #                              learnRate=learnRate,\n",
    "    #                              hiddenLayers=hiddenLayers,\n",
    "    #                              layerWidth=layerWidth,\n",
    "    #                              outputActivation=outputActivation,\n",
    "    #                              hiddenActivation=hiddenActivation,\n",
    "    #                              convActivation=convActivation,\n",
    "    #                              epochs=bestEpochs[0],\n",
    "    #                              optimiser=optimiser)\n",
    "\n",
    "    #    if val_acc > bestOptimiser[2]:\n",
    "    #        bestOptimiser = (optimiser, test_acc, val_acc)\n",
    "\n",
    "\n",
    "    # Output activation function\n",
    "    bestOutActivation = (\"selu\", 0, 0)\n",
    "    #for outputActivation in [\"relu\", \"sigmoid\", \"softmax\", \"softplus\", \"softsign\",\n",
    "    #                \"tanh\", \"selu\", \"elu\", \"exponential\"]:\n",
    "    #    test_acc, val_acc = model(xTrain, yTrain, xVal, yVal, xTest, yTest,\n",
    "    #                              inputShape=inputShape, outputShape=outputShape,\n",
    "    #                              lossFunction=lossFunction,\n",
    "    #                              learnRate=learnRate,\n",
    "    #                              hiddenLayers=hiddenLayers,\n",
    "    #                              layerWidth=layerWidth,\n",
    "    #                              outputActivation=outputActivation,\n",
    "    #                              hiddenActivation=hiddenActivation,\n",
    "    #                              convActivation=convActivation,\n",
    "    #                              epochs=bestEpochs[0],\n",
    "    #                              optimiser=bestOptimiser[0])\n",
    "\n",
    "    #    if val_acc > bestOutActivation[2]:\n",
    "    #        bestOutActivation = (outputActivation, test_acc, val_acc)\n",
    "\n",
    "\n",
    "    # Hidden Layer activation function\n",
    "    bestHiddenActivation = (\"softsign\", 0, 0)\n",
    "    #for hiddenActivation in [\"relu\", \"sigmoid\", \"softmax\", \"softplus\", \"softsign\",\n",
    "    #                \"tanh\", \"selu\", \"elu\", \"exponential\"]:\n",
    "    #    test_acc, val_acc = model(xTrain, yTrain, xVal, yVal, xTest, yTest,\n",
    "    #                              inputShape=inputShape, outputShape=outputShape,\n",
    "    #                              lossFunction=lossFunction,\n",
    "    #                              learnRate=learnRate,\n",
    "    #                              hiddenLayers=hiddenLayers,\n",
    "    #                              layerWidth=layerWidth,\n",
    "    #                              outputActivation=bestOutActivation[0],\n",
    "    #                              hiddenActivation=hiddenActivation,\n",
    "    #                              convActivation=convActivation,\n",
    "    #                              epochs=bestEpochs[0],\n",
    "    #                              optimiser=bestOptimiser[0])\n",
    "\n",
    "    #    if val_acc > bestHiddenActivation[2]:\n",
    "    #        bestHiddenActivation = (hiddenActivation, test_acc, val_acc)\n",
    "\n",
    "    \n",
    "    # Convolutional layer Activation Function\n",
    "    bestConvActivation = (\"relu\", 0, 0)\n",
    "    #if CNN:\n",
    "    #    for convActivation in [\"relu\", \"sigmoid\", \"softmax\", \"softplus\", \"softsign\",\n",
    "    #                    \"tanh\", \"selu\", \"elu\", \"exponential\"]:\n",
    "    #        test_acc, val_acc = CNNModelTest(xTrain, yTrain, xVal, yVal, xTest, yTest,\n",
    "    #                                         inputShape=inputShape, outputShape=outputShape,\n",
    "    #                                         lossFunction=lossFunction,\n",
    "    #                                        learnRate=learnRate,\n",
    "    #                                        hiddenLayers=hiddenLayers,\n",
    "    #                                        layerWidth=layerWidth,\n",
    "    #                                        outputActivation=bestOutActivation[0],\n",
    "    #                                        hiddenActivation=bestHiddenActivation[0],\n",
    "    #                                        convActivation=convActivation,\n",
    "    #                                        epochs=bestEpochs[0],\n",
    "    #                                        optimiser=bestOptimiser[0])\n",
    "\n",
    "    #        if val_acc > bestConvActivation[2]:\n",
    "    #            bestConvActivation = (convActivation, test_acc, val_acc)\n",
    "\n",
    "    \n",
    "    # Hidden Layers\n",
    "    bestHidden = (6, 0, 0)\n",
    "    #for hiddenLayers in range(1, 50):\n",
    "    #    test_acc, val_acc = model(xTrain, yTrain, xVal, yVal, xTest, yTest,\n",
    "    #                              inputShape=inputShape, outputShape=outputShape,\n",
    "    #                            lossFunction=lossFunction,\n",
    "    #                            learnRate=learnRate,\n",
    "    #                            hiddenLayers=hiddenLayers,\n",
    "    #                            layerWidth=layerWidth,\n",
    "    #                            outputActivation=bestOutActivation[0],\n",
    "    #                            hiddenActivation=bestHiddenActivation[0],\n",
    "    #                            convActivation=bestConvActivation[0],\n",
    "    #                            epochs=bestEpochs[0],\n",
    "    #                            optimiser=bestOptimiser[0])\n",
    "\n",
    "    #    if val_acc > bestHidden[2]:\n",
    "    #        bestHidden = (hiddenLayers, test_acc, val_acc)\n",
    "\n",
    "    #    elif val_acc < bestHidden[2] - 0.1:\n",
    "    #        break\n",
    "\n",
    "    \n",
    "    # Hidden Layers\n",
    "    bestWidth = (352, 0, 0)\n",
    "    accuracyList = []\n",
    "    for layerWidth in range(344, 360, 2):\n",
    "        test_acc, val_acc = model(xTrain, yTrain, xVal, yVal, xTest, yTest,\n",
    "                                  inputShape=inputShape, outputShape=outputShape,\n",
    "                                lossFunction=lossFunction,\n",
    "                                learnRate=learnRate,\n",
    "                                hiddenLayers=bestHidden[0],\n",
    "                                layerWidth=layerWidth,\n",
    "                                outputActivation=bestOutActivation[0],\n",
    "                                hiddenActivation=bestHiddenActivation[0],\n",
    "                                convActivation=bestConvActivation[0],\n",
    "                                epochs=bestEpochs[0],\n",
    "                                optimiser=bestOptimiser[0])\n",
    "    \n",
    "        accuracyList.append(val_acc)\n",
    "\n",
    "        if val_acc > bestWidth[2]:\n",
    "            bestWidth = (layerWidth, test_acc, val_acc)\n",
    "\n",
    "        #elif val_acc < bestWidth[2] - 0.1:\n",
    "        #    break\n",
    "\n",
    "\n",
    "    # Regularisations\n",
    "    bestAlpha = (0, 0, 0)\n",
    "    #alphas = np.logspace(-10, -2, 200)\n",
    "    #for alpha in alphas:\n",
    "    #  test_acc, val_acc = model(xTrain, yTrain, xVal, yVal, xTest, yTest,\n",
    "    #                          hiddenLayers=bestHidden[0],\n",
    "    #                          outputActivation=bestOutActivation[0],\n",
    "    #                          hiddenActivation=bestHiddenActivation[0],\n",
    "    #                          epochs=bestEpochs[0],\n",
    "    #                          optimiser=bestOptimiser[0],\n",
    "    #                          alpha=alpha)\n",
    "\n",
    "    #  if accuracy > bestAlpha[2]:\n",
    "    #   bestAlpha = (alpha, accuracy)\n",
    "\n",
    "    print(f\"Best Learning Rate: {bestLearnRate[0]}\")\n",
    "    print(f\"Best Hidden layers: {bestHidden[0]}\")\n",
    "    print(f\"Best Width: {bestWidth[0]}\")\n",
    "    print(f\"Best Optimiser: {bestOptimiser[0]}\")\n",
    "    print(f\"Best Output Activation: {bestOutActivation[0]}\")\n",
    "    print(f\"Best Hidden Activation: {bestHiddenActivation[0]}\")\n",
    "    print(f\"Best Conv Activation: {bestConvActivation[0]}\")\n",
    "    print(f\"Best Epochs: {bestEpochs[0]}\")\n",
    "    print(f\"Final Val accuracy: {bestHidden[2]}\")\n",
    "    print(f\"Final Test accuracy: {bestHidden[1]}\")\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(accuracyList, range(len(accuracyList)))\n",
    "    plt.show()\n",
    "\n",
    "    return {\"bestHidden\": bestHidden[0], \"bestOptimiser\": bestOptimiser[0],\n",
    "            \"bestOutActivation\": bestOutActivation[0],\n",
    "            \"bestHiddenActivation\": bestOutActivation[0],\n",
    "            \"bestEpochs\": bestEpochs[0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentation and Evaluation\n",
    "\n",
    "### MLP Model Evaluation\n",
    "\n",
    "Naive approach, best hyperparameters:\n",
    "\n",
    "Hidden layers: 1\n",
    "\n",
    "Optimiser: Adam\n",
    "\n",
    "Output Activation: SoftPlus\n",
    "\n",
    "Hidden Activation: Tanh\n",
    "\n",
    "Alpha:  5\n",
    "\n",
    "Epochs: 30\n",
    "\n",
    "Test accuracy: 0.8705999851226807"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "qOSRqMNMGoH3",
    "outputId": "31ded925-fc5d-4a5a-9692-8eb146673a6a"
   },
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(xTrainFull, yTrainFull), (xTest, yTest) = fashion_mnist.load_data()\n",
    "\n",
    "xVal, xTrain = xTrainFull[:5000] / 255.0, xTrainFull[5000:] / 255.0\n",
    "yVal, yTrain = yTrainFull[:5000], yTrainFull[5000:]\n",
    "\n",
    "# MLP model eval\n",
    "bestSettings = eval(MLPModelTest, xTrain, yTrain, xVal, yVal, xTest, yTest)\n",
    "print(bestSettings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Model Evaluation\n",
    "\n",
    "Naive approach, best hyperparameters:\n",
    "\n",
    "Hidden layers: 2\n",
    "\n",
    "Optimiser: Adamax\n",
    "\n",
    "Output Activation: elu\n",
    "\n",
    "Hidden Activation: sigmoid\n",
    "\n",
    "Conv Activation: relu\n",
    "\n",
    "Epochs: 30\n",
    "\n",
    "Test loss: 0.3733844459056854\n",
    "\n",
    "Test accuracy: 0.8733000159263611"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN model eval\n",
    "bestSettings = eval(CNNModelTest, xTrain, yTrain, xVal, yVal, xTest, yTest, CNN=True)\n",
    "print(bestSettings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar = keras.datasets.cifar10\n",
    "(xTrainFull, yTrainFull), (xTest, yTest) = cifar.load_data()\n",
    "\n",
    "xVal, xTrain = xTrainFull[:5000] / 255.0, xTrainFull[5000:] / 255.0\n",
    "yVal, yTrain = yTrainFull[:5000], yTrainFull[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hidden layers: 2\n",
    "#Optimiser: Adamax\n",
    "#Output Activation: elu\n",
    "#Hidden Activation: sigmoid\n",
    "#Conv Activation: relu\n",
    "#Epochs: 30\n",
    "#Test loss: 0.3733844459056854\n",
    "#Test accuracy: 0.8733000159263611\n",
    "\n",
    "test_acc, val_acc = CNNModelTest(xTrain, yTrain, xVal, yVal, xTest, yTest,\n",
    "                            learnRate=0.001,\n",
    "                          hiddenLayers=2,\n",
    "                          outputActivation=\"elu\",\n",
    "                          hiddenActivation=\"sigmoid\",\n",
    "                          convActivation=\"relu\",\n",
    "                          epochs=30,\n",
    "                          optimiser=keras.optimizers.Adamax,\n",
    "                          inputShape=(32, 32, 3))\n",
    "\n",
    "print(test_acc, val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T2.1 Tell-the-time Network\n",
    "\n",
    "## (a) Classification\n",
    "\n",
    "24 Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ClassCNN(xTrain, yTrain, xVal, yVal, xTest, yTest,\n",
    "                 learnRate=0.0001,\n",
    "                 hiddenLayers=1,\n",
    "                 layerWidth=64,\n",
    "                 outputActivation=\"softmax\",\n",
    "                 hiddenActivation=\"relu\",\n",
    "                 convActivation=\"relu\",\n",
    "                 optimiser=keras.optimizers.SGD,\n",
    "                 epochs=50,\n",
    "                 alpha=5,\n",
    "                 inputShape=(28, 28, 1),\n",
    "                 outputShape=10,\n",
    "                 lossFunction=keras.losses.SparseCategoricalCrossentropy(from_logits=True)):\n",
    "\n",
    "  CNNModel = Sequential()\n",
    "  CNNModel.add(Conv2D(32, (3, 3), activation=convActivation, input_shape=inputShape))\n",
    "  CNNModel.add(MaxPooling2D((2, 2)))\n",
    "  CNNModel.add(Conv2D(64, (3, 3), activation=convActivation))\n",
    "  CNNModel.add(MaxPooling2D((2, 2)))\n",
    "  CNNModel.add(Conv2D(128, (3, 3), activation=convActivation))\n",
    "  CNNModel.add(MaxPooling2D((2, 2)))\n",
    "  CNNModel.add(Conv2D(256, (3, 3), activation=convActivation))\n",
    "\n",
    "  CNNModel.add(Flatten())\n",
    "\n",
    "  for i in range(hiddenLayers):\n",
    "    CNNModel.add(Dense(layerWidth, activation=hiddenActivation))\n",
    "\n",
    "  CNNModel.add(Dense(outputShape, activation=outputActivation))\n",
    "\n",
    "  CNNModel.compile(optimizer=optimiser(learning_rate=learnRate),\n",
    "                loss=lossFunction,\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "  #CNNModel.summary()\n",
    "  history = CNNModel.fit(xTrain, yTrain, epochs=epochs,\n",
    "                      validation_data=(xVal, yVal))\n",
    "  \n",
    "  print(history.history)\n",
    "\n",
    "  pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "  plt.grid(True)\n",
    "  plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.show()\n",
    "\n",
    "  #print(CNNModel.predict(xVal))\n",
    "\n",
    "  val_loss, val_acc = CNNModel.evaluate(xVal,  yVal, verbose=2)\n",
    "  test_loss, test_acc = CNNModel.evaluate(xTest,  yTest, verbose=2)\n",
    "\n",
    "  print(f\"Learning Rate: {learnRate}\")\n",
    "  print(f\"Hidden layers: {hiddenLayers}\")\n",
    "  print(f\"Layer Width: {layerWidth}\")\n",
    "  print(f\"Optimiser: {optimiser}\")\n",
    "  print(f\"Output Activation: {outputActivation}\")\n",
    "  print(f\"Hidden Activation: {hiddenActivation}\")\n",
    "  print(f\"Conv Activation: {convActivation}\")\n",
    "  #print(f\"Alpha: {alpha}\")\n",
    "  print(f\"Epochs: {epochs}\")\n",
    "  print(f\"Validation loss: {val_loss}\")\n",
    "  print(f\"Validation accuracy: {val_acc}\")\n",
    "  print(f\"Test loss: {test_loss}\")\n",
    "  print(f\"Test accuracy: {test_acc}\")\n",
    "\n",
    "  return test_acc, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xSmallClock = np.load(\"data/75/images.npy\")\n",
    "ySmallClock = np.load(\"data/75/labels.npy\")\n",
    "\n",
    "ySmallClock = np.array([int(time[0] + (time[1] // 30)) for time in ySmallClock])  \n",
    "\n",
    "xSmallClock = np.reshape(xSmallClock, (-1, 75, 75, 1))\n",
    "xSmallClock = xSmallClock / 255.0\n",
    "\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(xSmallClock,\n",
    "                                                ySmallClock,\n",
    "                                                test_size=0.2)\n",
    "xVal, xTest, yVal, yTest = train_test_split(xTest, yTest, test_size=0.5)\n",
    "\n",
    "test_acc, val_acc = ClassCNN(xTrain, yTrain, xVal, yVal, xTest, yTest,\n",
    "                                 learnRate=0.001,\n",
    "                                hiddenLayers=2,\n",
    "                                outputActivation=\"softmax\",\n",
    "                                hiddenActivation=\"relu\",\n",
    "                                convActivation=\"relu\",\n",
    "                                epochs=30,\n",
    "                                optimiser=keras.optimizers.Adam,\n",
    "                                inputShape=(75, 75, 1),\n",
    "                                outputShape=24)\n",
    "\n",
    "print(test_acc, val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "720 Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 6.8069e-04 - loss: 6.5831 - val_accuracy: 0.0017 - val_loss: 6.5837\n",
      "Epoch 2/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 9.0682e-04 - loss: 6.5805 - val_accuracy: 0.0000e+00 - val_loss: 6.5874\n",
      "Epoch 3/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.0019 - loss: 6.5796 - val_accuracy: 0.0000e+00 - val_loss: 6.5897\n",
      "Epoch 4/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.0015 - loss: 6.5782 - val_accuracy: 0.0011 - val_loss: 6.5915\n",
      "Epoch 5/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.0014 - loss: 6.5781 - val_accuracy: 0.0000e+00 - val_loss: 6.5938\n",
      "Epoch 6/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.0014 - loss: 6.5777 - val_accuracy: 0.0000e+00 - val_loss: 6.5958\n",
      "Epoch 7/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.0018 - loss: 6.5774 - val_accuracy: 0.0000e+00 - val_loss: 6.5967\n",
      "Epoch 8/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.0010 - loss: 6.5774 - val_accuracy: 0.0000e+00 - val_loss: 6.5986\n",
      "Epoch 9/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.0015 - loss: 6.5769 - val_accuracy: 0.0000e+00 - val_loss: 6.6004\n",
      "Epoch 10/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.0012 - loss: 6.5765 - val_accuracy: 0.0000e+00 - val_loss: 6.6019\n",
      "Epoch 11/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 9.2897e-04 - loss: 6.5768 - val_accuracy: 0.0000e+00 - val_loss: 6.6034\n",
      "Epoch 12/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.0022 - loss: 6.5769 - val_accuracy: 0.0000e+00 - val_loss: 6.6043\n",
      "Epoch 13/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.0014 - loss: 6.5758 - val_accuracy: 0.0000e+00 - val_loss: 6.6045\n",
      "Epoch 14/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 8.7920e-04 - loss: 6.5759 - val_accuracy: 0.0000e+00 - val_loss: 6.6057\n",
      "Epoch 15/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.0027 - loss: 6.5758 - val_accuracy: 0.0000e+00 - val_loss: 6.6067\n",
      "Epoch 16/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.0012 - loss: 6.5762 - val_accuracy: 0.0000e+00 - val_loss: 6.6075\n",
      "Epoch 17/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.0023 - loss: 6.5768 - val_accuracy: 0.0000e+00 - val_loss: 6.6094\n",
      "Epoch 18/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.0027 - loss: 6.5757 - val_accuracy: 0.0000e+00 - val_loss: 6.6092\n",
      "Epoch 19/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.0016 - loss: 6.5757 - val_accuracy: 0.0000e+00 - val_loss: 6.6103\n",
      "Epoch 20/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.0016 - loss: 6.5745 - val_accuracy: 0.0000e+00 - val_loss: 6.6101\n",
      "Epoch 21/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.0016 - loss: 6.5746 - val_accuracy: 0.0000e+00 - val_loss: 6.6107\n",
      "Epoch 22/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.0020 - loss: 6.5748 - val_accuracy: 0.0000e+00 - val_loss: 6.6113\n",
      "Epoch 23/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.0027 - loss: 6.5730 - val_accuracy: 0.0000e+00 - val_loss: 6.6103\n",
      "Epoch 24/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.0020 - loss: 6.5725 - val_accuracy: 0.0000e+00 - val_loss: 6.6116\n",
      "Epoch 25/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.0022 - loss: 6.5724 - val_accuracy: 0.0000e+00 - val_loss: 6.6123\n",
      "Epoch 26/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.0031 - loss: 6.5686 - val_accuracy: 0.0000e+00 - val_loss: 6.6120\n",
      "Epoch 27/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.0016 - loss: 6.5648 - val_accuracy: 5.5556e-04 - val_loss: 6.6135\n",
      "Epoch 28/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.0032 - loss: 6.5583 - val_accuracy: 0.0000e+00 - val_loss: 6.6162\n",
      "Epoch 29/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.0024 - loss: 6.5524 - val_accuracy: 0.0011 - val_loss: 6.6175\n",
      "Epoch 30/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.0015 - loss: 6.5431 - val_accuracy: 0.0000e+00 - val_loss: 6.6185\n",
      "Epoch 31/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.0030 - loss: 6.5287 - val_accuracy: 5.5556e-04 - val_loss: 6.6100\n",
      "Epoch 32/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.0037 - loss: 6.5092 - val_accuracy: 0.0000e+00 - val_loss: 6.6044\n",
      "Epoch 33/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.0052 - loss: 6.4926 - val_accuracy: 0.0011 - val_loss: 6.5970\n",
      "Epoch 34/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.0051 - loss: 6.4668 - val_accuracy: 0.0022 - val_loss: 6.5749\n",
      "Epoch 35/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.0052 - loss: 6.4287 - val_accuracy: 0.0033 - val_loss: 6.5238\n",
      "Epoch 36/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.0064 - loss: 6.3750 - val_accuracy: 0.0028 - val_loss: 6.4318\n",
      "Epoch 37/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.0083 - loss: 6.2848 - val_accuracy: 0.0061 - val_loss: 6.3484\n",
      "Epoch 38/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.0075 - loss: 6.1658 - val_accuracy: 0.0050 - val_loss: 6.1202\n",
      "Epoch 39/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.0104 - loss: 5.9250 - val_accuracy: 0.0089 - val_loss: 5.7267\n",
      "Epoch 40/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.0169 - loss: 5.5504 - val_accuracy: 0.0122 - val_loss: 5.4333\n",
      "Epoch 41/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.0293 - loss: 5.2207 - val_accuracy: 0.0200 - val_loss: 5.0554\n",
      "Epoch 42/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.0371 - loss: 4.9364 - val_accuracy: 0.0233 - val_loss: 4.8587\n",
      "Epoch 43/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.0536 - loss: 4.6420 - val_accuracy: 0.0444 - val_loss: 4.4946\n",
      "Epoch 44/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.0683 - loss: 4.3580 - val_accuracy: 0.0494 - val_loss: 4.2358\n",
      "Epoch 45/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.0903 - loss: 4.0653 - val_accuracy: 0.0617 - val_loss: 3.9959\n",
      "Epoch 46/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.1062 - loss: 3.8209 - val_accuracy: 0.0900 - val_loss: 3.7485\n",
      "Epoch 47/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.1304 - loss: 3.5937 - val_accuracy: 0.1200 - val_loss: 3.5186\n",
      "Epoch 48/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.1397 - loss: 3.4116 - val_accuracy: 0.1306 - val_loss: 3.3974\n",
      "Epoch 49/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.1636 - loss: 3.2677 - val_accuracy: 0.1306 - val_loss: 3.2558\n",
      "Epoch 50/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.1928 - loss: 3.0962 - val_accuracy: 0.1711 - val_loss: 3.0974\n",
      "Epoch 51/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.2125 - loss: 2.9612 - val_accuracy: 0.1817 - val_loss: 2.9600\n",
      "Epoch 52/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.2251 - loss: 2.8402 - val_accuracy: 0.2111 - val_loss: 2.8739\n",
      "Epoch 53/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.2511 - loss: 2.7201 - val_accuracy: 0.2133 - val_loss: 2.7726\n",
      "Epoch 54/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.2784 - loss: 2.6104 - val_accuracy: 0.1817 - val_loss: 2.7377\n",
      "Epoch 55/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.2887 - loss: 2.5200 - val_accuracy: 0.2767 - val_loss: 2.5406\n",
      "Epoch 56/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.3205 - loss: 2.4205 - val_accuracy: 0.2350 - val_loss: 2.5757\n",
      "Epoch 57/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.3315 - loss: 2.3493 - val_accuracy: 0.2811 - val_loss: 2.4024\n",
      "Epoch 58/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.3592 - loss: 2.2668 - val_accuracy: 0.3044 - val_loss: 2.3243\n",
      "Epoch 59/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.3843 - loss: 2.1797 - val_accuracy: 0.3061 - val_loss: 2.3115\n",
      "Epoch 60/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.4028 - loss: 2.1132 - val_accuracy: 0.3272 - val_loss: 2.2359\n",
      "Epoch 61/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.4275 - loss: 2.0474 - val_accuracy: 0.3011 - val_loss: 2.2140\n",
      "Epoch 62/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.4427 - loss: 1.9836 - val_accuracy: 0.3117 - val_loss: 2.1479\n",
      "Epoch 63/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.4590 - loss: 1.9277 - val_accuracy: 0.3356 - val_loss: 2.1350\n",
      "Epoch 64/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.4620 - loss: 1.8878 - val_accuracy: 0.3806 - val_loss: 2.0158\n",
      "Epoch 65/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.4944 - loss: 1.8209 - val_accuracy: 0.3889 - val_loss: 1.9680\n",
      "Epoch 66/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5127 - loss: 1.7669 - val_accuracy: 0.4178 - val_loss: 1.9351\n",
      "Epoch 67/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5478 - loss: 1.7104 - val_accuracy: 0.3789 - val_loss: 1.9620\n",
      "Epoch 68/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5417 - loss: 1.6842 - val_accuracy: 0.4306 - val_loss: 1.8651\n",
      "Epoch 69/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5639 - loss: 1.6324 - val_accuracy: 0.3422 - val_loss: 1.9565\n",
      "Epoch 70/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5694 - loss: 1.5950 - val_accuracy: 0.4294 - val_loss: 1.8064\n",
      "Epoch 71/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5952 - loss: 1.5549 - val_accuracy: 0.3950 - val_loss: 1.8072\n",
      "Epoch 72/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.6204 - loss: 1.4981 - val_accuracy: 0.4633 - val_loss: 1.7366\n",
      "Epoch 73/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.6437 - loss: 1.4558 - val_accuracy: 0.4589 - val_loss: 1.7496\n",
      "Epoch 74/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.6392 - loss: 1.4257 - val_accuracy: 0.4978 - val_loss: 1.6701\n",
      "Epoch 75/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.6693 - loss: 1.3761 - val_accuracy: 0.4833 - val_loss: 1.6687\n",
      "Epoch 76/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.6838 - loss: 1.3409 - val_accuracy: 0.4717 - val_loss: 1.6377\n",
      "Epoch 77/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7017 - loss: 1.3017 - val_accuracy: 0.4594 - val_loss: 1.6429\n",
      "Epoch 78/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7110 - loss: 1.2661 - val_accuracy: 0.4883 - val_loss: 1.6047\n",
      "Epoch 79/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7268 - loss: 1.2380 - val_accuracy: 0.5161 - val_loss: 1.5607\n",
      "Epoch 80/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7402 - loss: 1.2028 - val_accuracy: 0.4633 - val_loss: 1.6377\n",
      "Epoch 81/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7533 - loss: 1.1677 - val_accuracy: 0.4567 - val_loss: 1.6601\n",
      "Epoch 82/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7547 - loss: 1.1457 - val_accuracy: 0.4994 - val_loss: 1.5397\n",
      "Epoch 83/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7688 - loss: 1.1149 - val_accuracy: 0.5017 - val_loss: 1.5160\n",
      "Epoch 84/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7850 - loss: 1.0757 - val_accuracy: 0.5472 - val_loss: 1.4498\n",
      "Epoch 85/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7977 - loss: 1.0440 - val_accuracy: 0.5539 - val_loss: 1.4371\n",
      "Epoch 86/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8069 - loss: 1.0195 - val_accuracy: 0.5533 - val_loss: 1.4277\n",
      "Epoch 87/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8235 - loss: 0.9841 - val_accuracy: 0.5322 - val_loss: 1.4417\n",
      "Epoch 88/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8203 - loss: 0.9628 - val_accuracy: 0.5356 - val_loss: 1.4385\n",
      "Epoch 89/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8340 - loss: 0.9362 - val_accuracy: 0.5567 - val_loss: 1.4136\n",
      "Epoch 90/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8492 - loss: 0.9032 - val_accuracy: 0.5894 - val_loss: 1.3620\n",
      "Epoch 91/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8499 - loss: 0.8901 - val_accuracy: 0.5478 - val_loss: 1.3942\n",
      "Epoch 92/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8673 - loss: 0.8489 - val_accuracy: 0.5483 - val_loss: 1.3997\n",
      "Epoch 93/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8708 - loss: 0.8323 - val_accuracy: 0.5456 - val_loss: 1.3702\n",
      "Epoch 94/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8711 - loss: 0.8143 - val_accuracy: 0.5644 - val_loss: 1.3523\n",
      "Epoch 95/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8953 - loss: 0.7743 - val_accuracy: 0.5739 - val_loss: 1.3337\n",
      "Epoch 96/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8943 - loss: 0.7575 - val_accuracy: 0.5922 - val_loss: 1.2995\n",
      "Epoch 97/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8977 - loss: 0.7374 - val_accuracy: 0.5806 - val_loss: 1.2985\n",
      "Epoch 98/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9085 - loss: 0.7097 - val_accuracy: 0.5711 - val_loss: 1.3329\n",
      "Epoch 99/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9211 - loss: 0.6796 - val_accuracy: 0.5583 - val_loss: 1.3396\n",
      "Epoch 100/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9157 - loss: 0.6683 - val_accuracy: 0.5983 - val_loss: 1.2634\n",
      "Epoch 101/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9340 - loss: 0.6380 - val_accuracy: 0.5572 - val_loss: 1.3313\n",
      "Epoch 102/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9393 - loss: 0.6161 - val_accuracy: 0.5944 - val_loss: 1.2495\n",
      "Epoch 103/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9382 - loss: 0.6089 - val_accuracy: 0.5689 - val_loss: 1.3159\n",
      "Epoch 104/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9428 - loss: 0.5905 - val_accuracy: 0.6144 - val_loss: 1.2280\n",
      "Epoch 105/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9443 - loss: 0.5757 - val_accuracy: 0.6072 - val_loss: 1.2434\n",
      "Epoch 106/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9402 - loss: 0.5684 - val_accuracy: 0.5900 - val_loss: 1.2471\n",
      "Epoch 107/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9569 - loss: 0.5352 - val_accuracy: 0.6117 - val_loss: 1.2472\n",
      "Epoch 108/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9541 - loss: 0.5270 - val_accuracy: 0.6100 - val_loss: 1.2086\n",
      "Epoch 109/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9626 - loss: 0.5007 - val_accuracy: 0.6161 - val_loss: 1.2052\n",
      "Epoch 110/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9698 - loss: 0.4796 - val_accuracy: 0.6072 - val_loss: 1.2137\n",
      "Epoch 111/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9735 - loss: 0.4616 - val_accuracy: 0.6022 - val_loss: 1.2475\n",
      "Epoch 112/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9671 - loss: 0.4593 - val_accuracy: 0.6089 - val_loss: 1.2093\n",
      "Epoch 113/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9731 - loss: 0.4459 - val_accuracy: 0.6206 - val_loss: 1.2061\n",
      "Epoch 114/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9784 - loss: 0.4283 - val_accuracy: 0.6128 - val_loss: 1.1970\n",
      "Epoch 115/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9775 - loss: 0.4135 - val_accuracy: 0.6017 - val_loss: 1.2492\n",
      "Epoch 116/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9842 - loss: 0.3971 - val_accuracy: 0.6428 - val_loss: 1.1602\n",
      "Epoch 117/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9833 - loss: 0.3837 - val_accuracy: 0.6300 - val_loss: 1.1793\n",
      "Epoch 118/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9897 - loss: 0.3663 - val_accuracy: 0.6194 - val_loss: 1.1796\n",
      "Epoch 119/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9865 - loss: 0.3588 - val_accuracy: 0.6156 - val_loss: 1.1978\n",
      "Epoch 120/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9899 - loss: 0.3528 - val_accuracy: 0.6250 - val_loss: 1.1861\n",
      "Epoch 121/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9895 - loss: 0.3426 - val_accuracy: 0.6117 - val_loss: 1.1977\n",
      "Epoch 122/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9908 - loss: 0.3232 - val_accuracy: 0.6256 - val_loss: 1.1788\n",
      "Epoch 123/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9954 - loss: 0.3065 - val_accuracy: 0.6267 - val_loss: 1.1759\n",
      "Epoch 124/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9952 - loss: 0.2972 - val_accuracy: 0.6256 - val_loss: 1.1778\n",
      "Epoch 125/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9958 - loss: 0.2936 - val_accuracy: 0.6189 - val_loss: 1.1698\n",
      "Epoch 126/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9975 - loss: 0.2793 - val_accuracy: 0.6233 - val_loss: 1.1706\n",
      "Epoch 127/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9977 - loss: 0.2726 - val_accuracy: 0.6239 - val_loss: 1.1919\n",
      "Epoch 128/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9961 - loss: 0.2677 - val_accuracy: 0.6133 - val_loss: 1.1935\n",
      "Epoch 129/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9980 - loss: 0.2577 - val_accuracy: 0.6217 - val_loss: 1.1875\n",
      "Epoch 130/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9978 - loss: 0.2523 - val_accuracy: 0.6344 - val_loss: 1.1617\n",
      "Epoch 131/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9984 - loss: 0.2412 - val_accuracy: 0.6267 - val_loss: 1.1723\n",
      "Epoch 132/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9986 - loss: 0.2356 - val_accuracy: 0.6156 - val_loss: 1.1985\n",
      "Epoch 133/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9989 - loss: 0.2277 - val_accuracy: 0.6317 - val_loss: 1.1749\n",
      "Epoch 134/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9987 - loss: 0.2228 - val_accuracy: 0.6256 - val_loss: 1.1765\n",
      "Epoch 135/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9989 - loss: 0.2165 - val_accuracy: 0.6200 - val_loss: 1.1936\n",
      "Epoch 136/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9992 - loss: 0.2114 - val_accuracy: 0.6333 - val_loss: 1.1720\n",
      "Epoch 137/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9997 - loss: 0.2035 - val_accuracy: 0.6272 - val_loss: 1.1916\n",
      "Epoch 138/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9999 - loss: 0.2021 - val_accuracy: 0.6228 - val_loss: 1.2069\n",
      "Epoch 139/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9992 - loss: 0.1998 - val_accuracy: 0.6294 - val_loss: 1.1791\n",
      "Epoch 140/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9997 - loss: 0.1926 - val_accuracy: 0.6333 - val_loss: 1.1856\n",
      "Epoch 141/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9999 - loss: 0.1856 - val_accuracy: 0.6167 - val_loss: 1.1944\n",
      "Epoch 142/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9999 - loss: 0.1806 - val_accuracy: 0.6306 - val_loss: 1.1812\n",
      "Epoch 143/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9999 - loss: 0.1800 - val_accuracy: 0.6272 - val_loss: 1.1827\n",
      "Epoch 144/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9998 - loss: 0.1729 - val_accuracy: 0.6222 - val_loss: 1.1867\n",
      "Epoch 145/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9999 - loss: 0.1683 - val_accuracy: 0.6300 - val_loss: 1.1897\n",
      "Epoch 146/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.1639 - val_accuracy: 0.6300 - val_loss: 1.1887\n",
      "Epoch 147/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9998 - loss: 0.1603 - val_accuracy: 0.6283 - val_loss: 1.1906\n",
      "Epoch 148/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.1580 - val_accuracy: 0.6283 - val_loss: 1.1959\n",
      "Epoch 149/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9997 - loss: 0.1557 - val_accuracy: 0.6317 - val_loss: 1.1987\n",
      "Epoch 150/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9999 - loss: 0.1499 - val_accuracy: 0.6272 - val_loss: 1.1897\n",
      "Epoch 151/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.1483 - val_accuracy: 0.6294 - val_loss: 1.2003\n",
      "Epoch 152/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.1442 - val_accuracy: 0.6272 - val_loss: 1.2030\n",
      "Epoch 153/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.1417 - val_accuracy: 0.6306 - val_loss: 1.2013\n",
      "Epoch 154/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.1379 - val_accuracy: 0.6322 - val_loss: 1.2024\n",
      "Epoch 155/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.1362 - val_accuracy: 0.6228 - val_loss: 1.2057\n",
      "Epoch 156/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.1331 - val_accuracy: 0.6250 - val_loss: 1.2152\n",
      "Epoch 157/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.1308 - val_accuracy: 0.6267 - val_loss: 1.2139\n",
      "Epoch 158/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.1269 - val_accuracy: 0.6250 - val_loss: 1.2169\n",
      "Epoch 159/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.1260 - val_accuracy: 0.6189 - val_loss: 1.2175\n",
      "Epoch 160/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.1228 - val_accuracy: 0.6272 - val_loss: 1.2187\n",
      "Epoch 161/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.1208 - val_accuracy: 0.6278 - val_loss: 1.2172\n",
      "Epoch 162/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.1190 - val_accuracy: 0.6233 - val_loss: 1.2187\n",
      "Epoch 163/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.1161 - val_accuracy: 0.6244 - val_loss: 1.2249\n",
      "Epoch 164/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.1148 - val_accuracy: 0.6250 - val_loss: 1.2250\n",
      "Epoch 165/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.1125 - val_accuracy: 0.6222 - val_loss: 1.2314\n",
      "Epoch 166/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.1106 - val_accuracy: 0.6228 - val_loss: 1.2342\n",
      "Epoch 167/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.1084 - val_accuracy: 0.6328 - val_loss: 1.2263\n",
      "Epoch 168/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.1068 - val_accuracy: 0.6206 - val_loss: 1.2368\n",
      "Epoch 169/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.1043 - val_accuracy: 0.6272 - val_loss: 1.2419\n",
      "Epoch 170/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.1037 - val_accuracy: 0.6239 - val_loss: 1.2426\n",
      "Epoch 171/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.1016 - val_accuracy: 0.6206 - val_loss: 1.2477\n",
      "Epoch 172/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.1000 - val_accuracy: 0.6211 - val_loss: 1.2513\n",
      "Epoch 173/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0982 - val_accuracy: 0.6267 - val_loss: 1.2480\n",
      "Epoch 174/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0972 - val_accuracy: 0.6194 - val_loss: 1.2490\n",
      "Epoch 175/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0952 - val_accuracy: 0.6211 - val_loss: 1.2547\n",
      "Epoch 176/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0930 - val_accuracy: 0.6250 - val_loss: 1.2521\n",
      "Epoch 177/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0925 - val_accuracy: 0.6217 - val_loss: 1.2590\n",
      "Epoch 178/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0910 - val_accuracy: 0.6233 - val_loss: 1.2623\n",
      "Epoch 179/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0901 - val_accuracy: 0.6206 - val_loss: 1.2617\n",
      "Epoch 180/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0881 - val_accuracy: 0.6228 - val_loss: 1.2615\n",
      "Epoch 181/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0870 - val_accuracy: 0.6228 - val_loss: 1.2671\n",
      "Epoch 182/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0862 - val_accuracy: 0.6217 - val_loss: 1.2696\n",
      "Epoch 183/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0846 - val_accuracy: 0.6172 - val_loss: 1.2668\n",
      "Epoch 184/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0835 - val_accuracy: 0.6172 - val_loss: 1.2750\n",
      "Epoch 185/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0823 - val_accuracy: 0.6211 - val_loss: 1.2733\n",
      "Epoch 186/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0806 - val_accuracy: 0.6150 - val_loss: 1.2758\n",
      "Epoch 187/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0803 - val_accuracy: 0.6189 - val_loss: 1.2738\n",
      "Epoch 188/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0789 - val_accuracy: 0.6161 - val_loss: 1.2790\n",
      "Epoch 189/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0781 - val_accuracy: 0.6161 - val_loss: 1.2840\n",
      "Epoch 190/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0766 - val_accuracy: 0.6200 - val_loss: 1.2891\n",
      "Epoch 191/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0761 - val_accuracy: 0.6133 - val_loss: 1.2850\n",
      "Epoch 192/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0752 - val_accuracy: 0.6189 - val_loss: 1.2873\n",
      "Epoch 193/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0740 - val_accuracy: 0.6167 - val_loss: 1.2840\n",
      "Epoch 194/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0731 - val_accuracy: 0.6139 - val_loss: 1.2928\n",
      "Epoch 195/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0720 - val_accuracy: 0.6161 - val_loss: 1.2916\n",
      "Epoch 196/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0715 - val_accuracy: 0.6200 - val_loss: 1.2947\n",
      "Epoch 197/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0702 - val_accuracy: 0.6172 - val_loss: 1.2974\n",
      "Epoch 198/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0693 - val_accuracy: 0.6194 - val_loss: 1.2953\n",
      "Epoch 199/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0681 - val_accuracy: 0.6172 - val_loss: 1.2961\n",
      "Epoch 200/200\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0678 - val_accuracy: 0.6161 - val_loss: 1.2988\n",
      "{'accuracy': [0.0006944444612599909, 0.0008333333535119891, 0.0015277777565643191, 0.0012499999720603228, 0.0010416667209938169, 0.0013194443890824914, 0.0015972221735864878, 0.0013888889225199819, 0.0015972221735864878, 0.0013888889225199819, 0.0011111111380159855, 0.0015972221735864878, 0.0014583333395421505, 0.0013194443890824914, 0.0020138889085501432, 0.0016666667070239782, 0.0018055555410683155, 0.0019444444915279746, 0.0017361111240461469, 0.0018749999580904841, 0.0016666667070239782, 0.0018749999580904841, 0.0023611111100763083, 0.0024305556435137987, 0.0024999999441206455, 0.0023611111100763083, 0.0018749999580904841, 0.0024305556435137987, 0.0024305556435137987, 0.0023611111100763083, 0.0030555555131286383, 0.0034722222480922937, 0.004652777686715126, 0.004861111287027597, 0.005625000223517418, 0.0062500000931322575, 0.006874999962747097, 0.008750000037252903, 0.012013888917863369, 0.01701388880610466, 0.027847222983837128, 0.03569444268941879, 0.05131944268941879, 0.06604166328907013, 0.08611111342906952, 0.10506944358348846, 0.12381944805383682, 0.14083333313465118, 0.1663888841867447, 0.18805555999279022, 0.20499999821186066, 0.22013889253139496, 0.2475000023841858, 0.2690972089767456, 0.28736111521720886, 0.3057639002799988, 0.3278472125530243, 0.3470138907432556, 0.3722916543483734, 0.39423611760139465, 0.4146527647972107, 0.42923611402511597, 0.4476388990879059, 0.4642361104488373, 0.47979167103767395, 0.5045833587646484, 0.5269444584846497, 0.5359722375869751, 0.5540972352027893, 0.5750694274902344, 0.5927777886390686, 0.6067361235618591, 0.6266666650772095, 0.6436111330986023, 0.6611111164093018, 0.6743055582046509, 0.6863194704055786, 0.6993749737739563, 0.7195833325386047, 0.7284722328186035, 0.7436110973358154, 0.7486110925674438, 0.7562500238418579, 0.7776389122009277, 0.7868055701255798, 0.7917361259460449, 0.8118055462837219, 0.8141666650772095, 0.8262500166893005, 0.8397916555404663, 0.8420833349227905, 0.8607639074325562, 0.862291693687439, 0.8677777647972107, 0.8883333206176758, 0.8879861235618591, 0.8914583325386047, 0.9019444584846497, 0.9115972518920898, 0.9130555391311646, 0.9267361164093018, 0.9311110973358154, 0.933263897895813, 0.9380555748939514, 0.9392361044883728, 0.9361110925674438, 0.9475694298744202, 0.9519444704055786, 0.9552083611488342, 0.9672916531562805, 0.96895831823349, 0.9665971994400024, 0.9709722399711609, 0.9797916412353516, 0.9751389026641846, 0.9820138812065125, 0.9818750023841858, 0.9870138764381409, 0.9860416650772095, 0.987500011920929, 0.9895833134651184, 0.9915972352027893, 0.9943749904632568, 0.9935416579246521, 0.9954166412353516, 0.9967361092567444, 0.9965972304344177, 0.995972216129303, 0.9975000023841858, 0.9977083206176758, 0.9984722137451172, 0.9983333349227905, 0.9990972280502319, 0.9984722137451172, 0.9992361068725586, 0.9992361068725586, 0.9990972280502319, 0.9994444251060486, 0.9987499713897705, 0.99979168176651, 0.9997222423553467, 0.9998611211776733, 0.9998611211776733, 0.9997222423553467, 0.99979168176651, 0.9999305605888367, 0.9998611211776733, 0.9999305605888367, 0.9999305605888367, 0.9999305605888367, 1.0, 0.9999305605888367, 1.0, 0.9999305605888367, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 'loss': [6.5838518142700195, 6.582094669342041, 6.580914497375488, 6.580277919769287, 6.579691410064697, 6.579365253448486, 6.578928470611572, 6.578769207000732, 6.578419208526611, 6.578092575073242, 6.578047752380371, 6.577775478363037, 6.577629089355469, 6.577464580535889, 6.577247619628906, 6.577095031738281, 6.57684850692749, 6.576732158660889, 6.576488018035889, 6.5762810707092285, 6.575999736785889, 6.575505256652832, 6.574885845184326, 6.573939800262451, 6.572243690490723, 6.5694499015808105, 6.565670013427734, 6.560988903045654, 6.554001808166504, 6.5445237159729, 6.530839920043945, 6.5128021240234375, 6.4933953285217285, 6.466170310974121, 6.423859596252441, 6.360060691833496, 6.272481918334961, 6.123221397399902, 5.849302291870117, 5.487485885620117, 5.1634979248046875, 4.886416435241699, 4.601369380950928, 4.30716609954834, 4.027228355407715, 3.7875826358795166, 3.575366973876953, 3.3989083766937256, 3.2353968620300293, 3.0815391540527344, 2.9517879486083984, 2.834752082824707, 2.7141876220703125, 2.609286308288574, 2.516719102859497, 2.4229793548583984, 2.3421459197998047, 2.266000747680664, 2.187152862548828, 2.1185781955718994, 2.0532021522521973, 1.9914826154708862, 1.9347516298294067, 1.8814129829406738, 1.8289440870285034, 1.7721608877182007, 1.72268807888031, 1.6858662366867065, 1.636459469795227, 1.5924968719482422, 1.5512927770614624, 1.5075256824493408, 1.4658831357955933, 1.4245136976242065, 1.3866682052612305, 1.3504828214645386, 1.3164197206497192, 1.278115153312683, 1.2411738634109497, 1.2135789394378662, 1.1782176494598389, 1.151297688484192, 1.1240202188491821, 1.0862386226654053, 1.0613324642181396, 1.035069465637207, 0.9952240586280823, 0.9709748029708862, 0.9444253444671631, 0.9153337478637695, 0.8980610966682434, 0.8565840125083923, 0.840377688407898, 0.8178235292434692, 0.780790388584137, 0.7655642628669739, 0.7490465641021729, 0.7187589406967163, 0.6935645937919617, 0.6772093772888184, 0.6482745409011841, 0.627203106880188, 0.6158708333969116, 0.5958885550498962, 0.5840657353401184, 0.5736850500106812, 0.5461820960044861, 0.5285663604736328, 0.5120469927787781, 0.48625850677490234, 0.46840178966522217, 0.4618381857872009, 0.44762712717056274, 0.42507830262184143, 0.42096757888793945, 0.3992440700531006, 0.38759854435920715, 0.3708934485912323, 0.36271071434020996, 0.3554825186729431, 0.34073278307914734, 0.3268505930900574, 0.3113461434841156, 0.30378779768943787, 0.2942642271518707, 0.2816755473613739, 0.27499935030937195, 0.2704315781593323, 0.2596384286880493, 0.2528077960014343, 0.2446214109659195, 0.23750415444374084, 0.22997871041297913, 0.22388668358325958, 0.21754297614097595, 0.21192623674869537, 0.20698082447052002, 0.20234282314777374, 0.20156335830688477, 0.1924932450056076, 0.18639710545539856, 0.18211983144283295, 0.17826461791992188, 0.1731271594762802, 0.16930332779884338, 0.16524231433868408, 0.1617051362991333, 0.15816247463226318, 0.15483631193637848, 0.15108981728553772, 0.14801618456840515, 0.1450222283601761, 0.14188285171985626, 0.13903522491455078, 0.13640457391738892, 0.13370126485824585, 0.13111461699008942, 0.12846513092517853, 0.12620969116687775, 0.12370134890079498, 0.12135531008243561, 0.11919759958982468, 0.11717408895492554, 0.11492694169282913, 0.11299944669008255, 0.1109502837061882, 0.1090831607580185, 0.10726048052310944, 0.10538133978843689, 0.10374519973993301, 0.10196776688098907, 0.10036376118659973, 0.09881962835788727, 0.0972534567117691, 0.095646932721138, 0.09421767294406891, 0.09284529834985733, 0.09137523919343948, 0.0900343731045723, 0.08870338648557663, 0.0874112918972969, 0.08623616397380829, 0.08498852699995041, 0.0837499126791954, 0.08259325474500656, 0.08145254850387573, 0.08035769313573837, 0.0792638510465622, 0.07816438376903534, 0.07712580263614655, 0.07615450769662857, 0.07514660805463791, 0.0741615891456604, 0.07325121760368347, 0.07229121029376984, 0.07143710553646088, 0.07051713019609451, 0.06970269232988358, 0.0688372403383255, 0.06796726584434509], 'val_accuracy': [0.0016666667070239782, 0.0, 0.0, 0.0011111111380159855, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0005555555690079927, 0.0, 0.0011111111380159855, 0.0, 0.0005555555690079927, 0.0, 0.0011111111380159855, 0.002222222276031971, 0.0033333334140479565, 0.0027777778450399637, 0.0061111110262572765, 0.004999999888241291, 0.008888889104127884, 0.012222222052514553, 0.019999999552965164, 0.023333333432674408, 0.04444444552063942, 0.049444444477558136, 0.061666667461395264, 0.09000000357627869, 0.11999999731779099, 0.13055555522441864, 0.13055555522441864, 0.1711111068725586, 0.18166667222976685, 0.21111111342906952, 0.2133333384990692, 0.18166667222976685, 0.27666667103767395, 0.23499999940395355, 0.28111112117767334, 0.30444443225860596, 0.30611109733581543, 0.32722222805023193, 0.301111102104187, 0.31166666746139526, 0.3355555534362793, 0.38055557012557983, 0.3888888955116272, 0.41777777671813965, 0.378888875246048, 0.4305555522441864, 0.3422222137451172, 0.42944443225860596, 0.39500001072883606, 0.4633333384990692, 0.4588888883590698, 0.4977777898311615, 0.4833333194255829, 0.4716666638851166, 0.45944443345069885, 0.4883333444595337, 0.5161111354827881, 0.4633333384990692, 0.4566666781902313, 0.49944445490837097, 0.5016666650772095, 0.5472221970558167, 0.5538889169692993, 0.5533333420753479, 0.5322222113609314, 0.5355555415153503, 0.5566666722297668, 0.5894444584846497, 0.5477777719497681, 0.5483333468437195, 0.5455555319786072, 0.5644444227218628, 0.573888897895813, 0.5922222137451172, 0.5805555582046509, 0.5711110830307007, 0.5583333373069763, 0.5983333587646484, 0.5572222471237183, 0.5944444537162781, 0.5688889026641846, 0.6144444346427917, 0.6072221994400024, 0.5899999737739563, 0.6116666793823242, 0.6100000143051147, 0.6161110997200012, 0.6072221994400024, 0.602222204208374, 0.6088888645172119, 0.620555579662323, 0.6127777695655823, 0.6016666889190674, 0.6427778005599976, 0.6299999952316284, 0.6194444298744202, 0.6155555844306946, 0.625, 0.6116666793823242, 0.6255555748939514, 0.6266666650772095, 0.6255555748939514, 0.6188889145851135, 0.6233333349227905, 0.6238889098167419, 0.6133333444595337, 0.621666669845581, 0.6344444155693054, 0.6266666650772095, 0.6155555844306946, 0.6316666603088379, 0.6255555748939514, 0.6200000047683716, 0.6333333253860474, 0.6272222399711609, 0.6227777600288391, 0.629444420337677, 0.6333333253860474, 0.6166666746139526, 0.6305555701255798, 0.6272222399711609, 0.6222222447395325, 0.6299999952316284, 0.6299999952316284, 0.628333330154419, 0.628333330154419, 0.6316666603088379, 0.6272222399711609, 0.629444420337677, 0.6272222399711609, 0.6305555701255798, 0.6322222352027893, 0.6227777600288391, 0.625, 0.6266666650772095, 0.625, 0.6188889145851135, 0.6272222399711609, 0.6277777552604675, 0.6233333349227905, 0.6244444251060486, 0.625, 0.6222222447395325, 0.6227777600288391, 0.632777750492096, 0.620555579662323, 0.6272222399711609, 0.6238889098167419, 0.620555579662323, 0.6211110949516296, 0.6266666650772095, 0.6194444298744202, 0.6211110949516296, 0.625, 0.621666669845581, 0.6233333349227905, 0.620555579662323, 0.6227777600288391, 0.6227777600288391, 0.621666669845581, 0.617222249507904, 0.617222249507904, 0.6211110949516296, 0.6150000095367432, 0.6188889145851135, 0.6161110997200012, 0.6161110997200012, 0.6200000047683716, 0.6133333444595337, 0.6188889145851135, 0.6166666746139526, 0.6138888597488403, 0.6161110997200012, 0.6200000047683716, 0.617222249507904, 0.6194444298744202, 0.617222249507904, 0.6161110997200012], 'val_loss': [6.583740234375, 6.587375164031982, 6.589695453643799, 6.591518402099609, 6.593825340270996, 6.595764636993408, 6.596693992614746, 6.598644256591797, 6.600438594818115, 6.601869106292725, 6.603362083435059, 6.604268550872803, 6.6044793128967285, 6.605710029602051, 6.606722354888916, 6.607484817504883, 6.6093549728393555, 6.609215259552002, 6.610301494598389, 6.610070705413818, 6.610724449157715, 6.61129903793335, 6.610265731811523, 6.611588001251221, 6.612276077270508, 6.612002372741699, 6.613475322723389, 6.616196632385254, 6.617527008056641, 6.618455410003662, 6.610040187835693, 6.604443550109863, 6.597044944763184, 6.574948787689209, 6.523752689361572, 6.431834697723389, 6.348429203033447, 6.1201653480529785, 5.726728439331055, 5.43332052230835, 5.055401802062988, 4.858653545379639, 4.494621753692627, 4.235770225524902, 3.995891809463501, 3.7485463619232178, 3.5185887813568115, 3.397448778152466, 3.2558135986328125, 3.09741473197937, 2.959960460662842, 2.8738889694213867, 2.772568702697754, 2.7377023696899414, 2.5405733585357666, 2.575711727142334, 2.4023778438568115, 2.3243229389190674, 2.311487913131714, 2.2358973026275635, 2.2139694690704346, 2.1478703022003174, 2.1349971294403076, 2.015821695327759, 1.967982292175293, 1.9350858926773071, 1.962003231048584, 1.8651171922683716, 1.956520438194275, 1.806416392326355, 1.8072264194488525, 1.7365518808364868, 1.7496014833450317, 1.6701099872589111, 1.6686949729919434, 1.637674331665039, 1.6429401636123657, 1.6047031879425049, 1.5606788396835327, 1.6376510858535767, 1.6600775718688965, 1.539685845375061, 1.516034483909607, 1.4498285055160522, 1.4370687007904053, 1.4276585578918457, 1.4417486190795898, 1.4384874105453491, 1.413602352142334, 1.3620165586471558, 1.3941690921783447, 1.3997094631195068, 1.3701637983322144, 1.3522770404815674, 1.3337211608886719, 1.299522876739502, 1.2984881401062012, 1.3329154253005981, 1.3396339416503906, 1.2633925676345825, 1.3312615156173706, 1.2495381832122803, 1.3159087896347046, 1.2280151844024658, 1.2434242963790894, 1.24709951877594, 1.2471996545791626, 1.2085728645324707, 1.2051852941513062, 1.2137051820755005, 1.2474870681762695, 1.2092690467834473, 1.206089735031128, 1.1970088481903076, 1.2491737604141235, 1.1601778268814087, 1.1793397665023804, 1.179626226425171, 1.1977876424789429, 1.1860798597335815, 1.197686791419983, 1.1788283586502075, 1.175900936126709, 1.1778181791305542, 1.169814944267273, 1.1706442832946777, 1.1919175386428833, 1.1935169696807861, 1.1875087022781372, 1.1616679430007935, 1.1723134517669678, 1.1984882354736328, 1.1749293804168701, 1.1764785051345825, 1.193627119064331, 1.172034502029419, 1.1915992498397827, 1.2069453001022339, 1.1791348457336426, 1.1855521202087402, 1.1944423913955688, 1.181174874305725, 1.1827270984649658, 1.1866631507873535, 1.1896759271621704, 1.1887072324752808, 1.190599799156189, 1.1958818435668945, 1.198730707168579, 1.1896843910217285, 1.2003037929534912, 1.203018307685852, 1.2013192176818848, 1.202418327331543, 1.2056914567947388, 1.215154767036438, 1.2139487266540527, 1.2168561220169067, 1.217512607574463, 1.2187299728393555, 1.217191457748413, 1.218658447265625, 1.2248879671096802, 1.2250045537948608, 1.2314451932907104, 1.2342314720153809, 1.2263333797454834, 1.2368130683898926, 1.2419037818908691, 1.2426238059997559, 1.2477422952651978, 1.2512588500976562, 1.248008370399475, 1.2490438222885132, 1.2547314167022705, 1.2521051168441772, 1.2589738368988037, 1.2622895240783691, 1.2616844177246094, 1.2615320682525635, 1.267054557800293, 1.2696247100830078, 1.2668424844741821, 1.275036334991455, 1.273332118988037, 1.2757545709609985, 1.2737669944763184, 1.2790330648422241, 1.284015417098999, 1.2891380786895752, 1.2849527597427368, 1.2872846126556396, 1.2840144634246826, 1.2928192615509033, 1.291603922843933, 1.2946574687957764, 1.2973617315292358, 1.2952920198440552, 1.296093225479126, 1.2987993955612183]}\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 - 0s - 981us/step - accuracy: 0.6161 - loss: 1.2988\n",
      "57/57 - 0s - 969us/step - accuracy: 0.6439 - loss: 1.2016\n",
      "Learning Rate: 0.01\n",
      "Hidden layers: 6\n",
      "Layer Width: 346\n",
      "Optimiser: <class 'keras.src.optimizers.adagrad.Adagrad'>\n",
      "Output Activation: selu\n",
      "Hidden Activation: softsign\n",
      "Conv Activation: relu\n",
      "Epochs: 200\n",
      "Validation loss: 1.2987993955612183\n",
      "Validation accuracy: 0.6161110997200012\n",
      "Test loss: 1.201573371887207\n",
      "Test accuracy: 0.6438888907432556\n",
      "0.6438888907432556 0.6161110997200012\n"
     ]
    }
   ],
   "source": [
    "xSmallClock = np.load(\"data/75/images.npy\")\n",
    "ySmallClock = np.load(\"data/75/labels.npy\")\n",
    "\n",
    "ySmallClock = np.array([int((time[0] * 60) + time[1]) for time in ySmallClock])  \n",
    "\n",
    "xSmallClock = np.reshape(xSmallClock, (-1, 75, 75, 1))\n",
    "xSmallClock = xSmallClock / 255.0\n",
    "\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(xSmallClock,\n",
    "                                                ySmallClock,\n",
    "                                                test_size=0.2)\n",
    "xVal, xTest, yVal, yTest = train_test_split(xTest, yTest, test_size=0.5)\n",
    "\n",
    "test_acc, val_acc = ClassCNN(xTrain, yTrain, xVal, yVal, xTest, yTest,\n",
    "                         learnRate=0.01,\n",
    "                         hiddenLayers=6,\n",
    "                         layerWidth=346,\n",
    "                         outputActivation=\"selu\",\n",
    "                         hiddenActivation=\"softsign\",\n",
    "                         convActivation=\"relu\",\n",
    "                         epochs=200,\n",
    "                         optimiser=keras.optimizers.Adagrad,\n",
    "                         inputShape=(75, 75, 1),\n",
    "                         outputShape=720)\n",
    "\n",
    "print(test_acc, val_acc)\n",
    "\n",
    "#bestSettings = eval(ClassCNN, xTrain, yTrain, xVal, yVal, xTest, yTest,\n",
    "#                          learnRate=0.01,\n",
    "#                          hiddenLayers=6,\n",
    "#                          layerWidth=512,\n",
    "#                          outputActivation=\"elu\",\n",
    "#                          hiddenActivation=\"elu\",\n",
    "#                          convActivation=\"relu\",\n",
    "#                          epochs=200,\n",
    "#                          optimiser=keras.optimizers.Adagrad,\n",
    "#                          inputShape=(75, 75, 1),\n",
    "#                          outputShape=720,\n",
    "#                          lossFunction=keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
    "\n",
    "#print(bestSettings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RegressionCNN(xTrain, yTrain, xVal, yVal, xTest, yTest,\n",
    "                 learnRate=0.0001,\n",
    "                 hiddenLayers=1,\n",
    "                 layerWidth=64,\n",
    "                 outputActivation=\"softmax\",\n",
    "                 hiddenActivation=\"relu\",\n",
    "                 convActivation=\"relu\",\n",
    "                 optimiser=keras.optimizers.SGD,\n",
    "                 epochs=50,\n",
    "                 alpha=5,\n",
    "                 inputShape=(28, 28, 1),\n",
    "                 outputShape=10,\n",
    "                 lossFunction=keras.losses.SparseCategoricalCrossentropy(from_logits=True)):\n",
    "\n",
    "  CNNModel = Sequential()\n",
    "  CNNModel.add(Conv2D(32, (3, 3), activation=convActivation, input_shape=inputShape))\n",
    "  CNNModel.add(MaxPooling2D((2, 2)))\n",
    "  CNNModel.add(Conv2D(16, (3, 3), activation=convActivation))\n",
    "  CNNModel.add(MaxPooling2D((2, 2)))\n",
    "  CNNModel.add(Conv2D(8, (3, 3), activation=convActivation))\n",
    "\n",
    "  CNNModel.add(Flatten())\n",
    "\n",
    "  for i in range(hiddenLayers):\n",
    "    CNNModel.add(Dense(layerWidth, activation=hiddenActivation))\n",
    "\n",
    "  CNNModel.add(Dense(4, activation=hiddenActivation))\n",
    "\n",
    "  CNNModel.add(Dense(outputShape, activation=outputActivation))\n",
    "\n",
    "  CNNModel.compile(optimizer=optimiser(learning_rate=learnRate),\n",
    "                loss=lossFunction,\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "  #CNNModel.summary()\n",
    "  history = CNNModel.fit(xTrain, yTrain, epochs=epochs,\n",
    "                      validation_data=(xVal, yVal))\n",
    "  \n",
    "  print(history.history)\n",
    "\n",
    "  pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "  plt.grid(True)\n",
    "  plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.show()\n",
    "\n",
    "  #print(CNNModel.predict(xVal))\n",
    "\n",
    "  val_loss, val_acc = CNNModel.evaluate(xVal,  yVal, verbose=2)\n",
    "  test_loss, test_acc = CNNModel.evaluate(xTest,  yTest, verbose=2)\n",
    "\n",
    "  print(f\"Learning Rate: {learnRate}\")\n",
    "  print(f\"Hidden layers: {hiddenLayers}\")\n",
    "  print(f\"Layer Width: {layerWidth}\")\n",
    "  print(f\"Optimiser: {optimiser}\")\n",
    "  print(f\"Output Activation: {outputActivation}\")\n",
    "  print(f\"Hidden Activation: {hiddenActivation}\")\n",
    "  print(f\"Conv Activation: {convActivation}\")\n",
    "  #print(f\"Alpha: {alpha}\")\n",
    "  print(f\"Epochs: {epochs}\")\n",
    "  print(f\"Validation loss: {val_loss}\")\n",
    "  print(f\"Validation accuracy: {val_acc}\")\n",
    "  print(f\"Test loss: {test_loss}\")\n",
    "  print(f\"Test accuracy: {test_acc}\")\n",
    "\n",
    "  return test_acc, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class circularLoss(Loss):\n",
    "    def call(self, y_true, y_pred):\n",
    "        return tf.reduce_mean(tf.abs(tf.atan2(tf.sin(y_true - y_pred), tf.cos(y_true - y_pred))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[130 134 141 ... 147 143 146]\n",
      "  [142 136 122 ... 150 151 153]\n",
      "  [140 129 143 ... 141 141 145]\n",
      "  ...\n",
      "  [135 111  74 ... 106 130 135]\n",
      "  [ 81 100 109 ... 129 104 132]\n",
      "  [ 92 123 120 ... 103  84  80]]\n",
      "\n",
      " [[138 155 148 ... 136 136 128]\n",
      "  [138 142 138 ... 126 121 134]\n",
      "  [125 143 143 ... 130 139 135]\n",
      "  ...\n",
      "  [101  79  65 ...  94  98  98]\n",
      "  [ 75  70  84 ...  95 100  90]\n",
      "  [ 80 104 107 ...  96 103  91]]\n",
      "\n",
      " [[137 131 148 ... 153 157 157]\n",
      "  [145 142 141 ... 150 153 153]\n",
      "  [139 143 146 ... 149 148 145]\n",
      "  ...\n",
      "  [ 97 114 121 ...  91 101 104]\n",
      "  [121 121 100 ... 100  85  76]\n",
      "  [134 132 112 ...  95  97  95]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[128 124 133 ... 146 147 148]\n",
      "  [141 144 149 ... 152 152 152]\n",
      "  [132 126 125 ... 147 149 148]\n",
      "  ...\n",
      "  [ 61  73  94 ...  91 100 105]\n",
      "  [135 118  96 ... 129 122 137]\n",
      "  [ 55  64  86 ... 136 108 146]]\n",
      "\n",
      " [[133 142 144 ... 101 104  89]\n",
      "  [118 127 132 ...  94 105 104]\n",
      "  [133 128 137 ... 104  99 113]\n",
      "  ...\n",
      "  [135  95  76 ...  64  81  68]\n",
      "  [149  95  77 ...  74  62  56]\n",
      "  [126  89  81 ...  98  86  71]]\n",
      "\n",
      " [[103 111 101 ... 100  98 103]\n",
      "  [116 117 117 ...  94  99  96]\n",
      "  [ 96  96 103 ... 114 103  98]\n",
      "  ...\n",
      "  [139 135 140 ... 128 113 125]\n",
      "  [133 136 141 ... 108 116 126]\n",
      "  [121 126 130 ... 120 112 113]]]\n",
      "[ 0.          0.          0.         ... 11.98333333 11.98333333\n",
      " 11.98333333]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/home/s4451856/miniconda3/envs/IDL/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/125\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0014 - loss: 1.5913 - val_accuracy: 0.0017 - val_loss: 1.5445\n",
      "Epoch 2/125\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.0011 - loss: 1.5405 - val_accuracy: 0.0017 - val_loss: 1.5226\n",
      "Epoch 3/125\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.0011 - loss: 1.4867 - val_accuracy: 0.0017 - val_loss: 1.5226\n",
      "Epoch 4/125\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.0012 - loss: 1.5135 - val_accuracy: 0.0017 - val_loss: 1.5232\n",
      "Epoch 5/125\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 9.6638e-04 - loss: 1.4864 - val_accuracy: 0.0017 - val_loss: 1.5232\n",
      "Epoch 6/125\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.0015 - loss: 1.5104 - val_accuracy: 0.0017 - val_loss: 1.5227\n",
      "Epoch 7/125\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.0013 - loss: 1.4986 - val_accuracy: 0.0017 - val_loss: 1.5227\n",
      "Epoch 8/125\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.0015 - loss: 1.5033 - val_accuracy: 0.0017 - val_loss: 1.5235\n",
      "Epoch 9/125\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.0014 - loss: 1.4973 - val_accuracy: 0.0017 - val_loss: 1.5229\n",
      "Epoch 10/125\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.0014 - loss: 1.4997 - val_accuracy: 0.0017 - val_loss: 1.5239\n",
      "Epoch 11/125\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.0015 - loss: 1.5062 - val_accuracy: 0.0017 - val_loss: 1.5226\n",
      "Epoch 12/125\n",
      "\u001b[1m399/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0012 - loss: 1.5174"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 17\u001b[0m\n\u001b[1;32m     12\u001b[0m xTrain, xTest, yTrain, yTest \u001b[38;5;241m=\u001b[39m train_test_split(xSmallClock,\n\u001b[1;32m     13\u001b[0m                                                 ySmallClock,\n\u001b[1;32m     14\u001b[0m                                                 test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m)\n\u001b[1;32m     15\u001b[0m xVal, xTest, yVal, yTest \u001b[38;5;241m=\u001b[39m train_test_split(xTest, yTest, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m test_acc, val_acc \u001b[38;5;241m=\u001b[39m RegressionCNN(xTrain, yTrain, xVal, yVal, xTest, yTest,\n\u001b[1;32m     18\u001b[0m                                  learnRate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m,\n\u001b[1;32m     19\u001b[0m                                 hiddenLayers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     20\u001b[0m                                 layerWidth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,\n\u001b[1;32m     21\u001b[0m                                 outputActivation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mselu\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     22\u001b[0m                                 hiddenActivation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftsign\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     23\u001b[0m                                 convActivation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     24\u001b[0m                                 epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m125\u001b[39m,\n\u001b[1;32m     25\u001b[0m                                 optimiser\u001b[38;5;241m=\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam,\n\u001b[1;32m     26\u001b[0m                                 inputShape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m75\u001b[39m, \u001b[38;5;241m75\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     27\u001b[0m                                 outputShape\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     28\u001b[0m                                 lossFunction\u001b[38;5;241m=\u001b[39mcircularLoss)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(test_acc, val_acc)\n",
      "Cell \u001b[0;32mIn[12], line 36\u001b[0m, in \u001b[0;36mRegressionCNN\u001b[0;34m(xTrain, yTrain, xVal, yVal, xTest, yTest, learnRate, hiddenLayers, layerWidth, outputActivation, hiddenActivation, convActivation, optimiser, epochs, alpha, inputShape, outputShape, lossFunction)\u001b[0m\n\u001b[1;32m     31\u001b[0m CNNModel\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39moptimiser(learning_rate\u001b[38;5;241m=\u001b[39mlearnRate),\n\u001b[1;32m     32\u001b[0m               loss\u001b[38;5;241m=\u001b[39mlossFunction,\n\u001b[1;32m     33\u001b[0m               metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m#CNNModel.summary()\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m history \u001b[38;5;241m=\u001b[39m CNNModel\u001b[38;5;241m.\u001b[39mfit(xTrain, yTrain, epochs\u001b[38;5;241m=\u001b[39mepochs,\n\u001b[1;32m     37\u001b[0m                     validation_data\u001b[38;5;241m=\u001b[39m(xVal, yVal))\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(history\u001b[38;5;241m.\u001b[39mhistory)\n\u001b[1;32m     41\u001b[0m pd\u001b[38;5;241m.\u001b[39mDataFrame(history\u001b[38;5;241m.\u001b[39mhistory)\u001b[38;5;241m.\u001b[39mplot(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/IDL/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/IDL/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:320\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[1;32m    319\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 320\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[1;32m    321\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m~/miniconda3/envs/IDL/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/IDL/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/miniconda3/envs/IDL/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m tracing_compilation\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[1;32m    879\u001b[0m     args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config\n\u001b[1;32m    880\u001b[0m )\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/IDL/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function\u001b[38;5;241m.\u001b[39m_call_flat(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[38;5;241m=\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs\n\u001b[1;32m    141\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/IDL/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function\u001b[38;5;241m.\u001b[39mcall_preflattened(args)\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/miniconda3/envs/IDL/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_flat(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/IDL/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    253\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    254\u001b[0m         \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mflat_outputs),\n\u001b[1;32m    255\u001b[0m     )\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/IDL/lib/python3.12/site-packages/tensorflow/python/eager/context.py:1552\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1550\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1552\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[1;32m   1553\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1554\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   1555\u001b[0m       inputs\u001b[38;5;241m=\u001b[39mtensor_inputs,\n\u001b[1;32m   1556\u001b[0m       attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[1;32m   1557\u001b[0m       ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1558\u001b[0m   )\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1560\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1561\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1562\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1566\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1567\u001b[0m   )\n",
      "File \u001b[0;32m~/miniconda3/envs/IDL/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "xSmallClock = np.load(\"data/75/images.npy\")\n",
    "ySmallClock = np.load(\"data/75/labels.npy\")\n",
    "\n",
    "ySmallClock = np.array([float(time[0] + (time[1] / 60)) for time in ySmallClock])\n",
    "\n",
    "print(xSmallClock)\n",
    "print(ySmallClock)\n",
    "\n",
    "xSmallClock = np.reshape(xSmallClock, (-1, 75, 75, 1))\n",
    "xSmallClock = xSmallClock / 255.0\n",
    "    \n",
    "xTrain, xTest, yTrain, yTest = train_test_split(xSmallClock,\n",
    "                                                ySmallClock,\n",
    "                                                test_size=0.2)\n",
    "xVal, xTest, yVal, yTest = train_test_split(xTest, yTest, test_size=0.5)\n",
    "\n",
    "test_acc, val_acc = RegressionCNN(xTrain, yTrain, xVal, yVal, xTest, yTest,\n",
    "                                 learnRate=0.001,\n",
    "                                hiddenLayers=2,\n",
    "                                layerWidth=8,\n",
    "                                outputActivation=\"selu\",\n",
    "                                hiddenActivation=\"softsign\",\n",
    "                                convActivation=\"relu\",\n",
    "                                epochs=125,\n",
    "                                optimiser=keras.optimizers.Adam,\n",
    "                                inputShape=(75, 75, 1),\n",
    "                                outputShape=1,\n",
    "                                lossFunction=circularLoss)\n",
    "\n",
    "print(test_acc, val_acc)\n",
    "\n",
    "#bestSettings = eval(CNNModelTest, xTrain, yTrain, xVal, yVal, xTest, yTest, CNN=True,\n",
    "#                    inputShape=(75, 75, 1), outputShape=1, lossFunction=circularLoss)\n",
    "#print(bestSettings)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
